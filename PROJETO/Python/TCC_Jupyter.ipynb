{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import de Bibliotecas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manipulação e visualização de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from scipy.io import arff\n",
    "from sklearn.cluster import AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classes para pré-processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funções de avaliação dos modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (accuracy_score, \n",
    "                             confusion_matrix, \n",
    "                             ConfusionMatrixDisplay, \n",
    "                             roc_curve,RocCurveDisplay,\n",
    "                             f1_score, \n",
    "                             classification_report, \n",
    "                             mean_squared_error, \n",
    "                             r2_score,\n",
    "                             precision_score)\n",
    "from sklearn.model_selection import (train_test_split, \n",
    "                                     KFold, \n",
    "                                     LeaveOneOut, \n",
    "                                     StratifiedKFold, \n",
    "                                     GridSearchCV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classes do modelo de aprendizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import (GaussianNB, \n",
    "                                 BernoulliNB, \n",
    "                                 MultinomialNB)\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import (DecisionTreeClassifier, \n",
    "                          DecisionTreeRegressor, \n",
    "                          plot_tree)\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lib de Warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declaração da variável com folder e arquivo do DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_dataset = ('DATASET/dataset_6_letter.arff')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leitura do DATASET e respectivo print em tela"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       x-box  y-box  width  high  onpix  x-bar  y-bar  x2bar  y2bar  xybar  \\\n",
      "0        2.0    4.0    4.0   3.0    2.0    7.0    8.0    2.0    9.0   11.0   \n",
      "1        4.0    7.0    5.0   5.0    5.0    5.0    9.0    6.0    4.0    8.0   \n",
      "2        7.0   10.0    8.0   7.0    4.0    8.0    8.0    5.0   10.0   11.0   \n",
      "3        4.0    9.0    5.0   7.0    4.0    7.0    7.0   13.0    1.0    7.0   \n",
      "4        6.0    7.0    8.0   5.0    4.0    7.0    6.0    3.0    7.0   10.0   \n",
      "...      ...    ...    ...   ...    ...    ...    ...    ...    ...    ...   \n",
      "19995    5.0   10.0    5.0   8.0    3.0    4.0   10.0    7.0    8.0   12.0   \n",
      "19996    4.0    7.0    6.0   5.0    3.0    7.0    8.0    2.0   10.0   12.0   \n",
      "19997    4.0    8.0    4.0   6.0    4.0    7.0    8.0    7.0    4.0   10.0   \n",
      "19998    4.0   11.0    4.0   8.0    3.0    0.0    2.0    4.0    6.0    1.0   \n",
      "19999    5.0    9.0    6.0  11.0    6.0    8.0    7.0    6.0    3.0    8.0   \n",
      "\n",
      "       x2ybr  xy2br  x-ege  xegvy  y-ege  yegvx class  \n",
      "0        7.0    7.0    1.0    8.0    5.0    6.0  b'Z'  \n",
      "1        7.0    9.0    2.0    9.0    7.0   10.0  b'P'  \n",
      "2        2.0    8.0    2.0    5.0    5.0   10.0  b'S'  \n",
      "3        6.0    8.0    3.0    8.0    0.0    8.0  b'H'  \n",
      "4        7.0    9.0    3.0    8.0    3.0    7.0  b'H'  \n",
      "...      ...    ...    ...    ...    ...    ...   ...  \n",
      "19995   10.0    9.0    2.0    9.0    2.0    6.0  b'C'  \n",
      "19996    6.0    8.0    1.0    9.0    6.0    8.0  b'Z'  \n",
      "19997    7.0    6.0    3.0    9.0    3.0    7.0  b'O'  \n",
      "19998    0.0    7.0    0.0    8.0    0.0    8.0  b'L'  \n",
      "19999    8.0   11.0    3.0    8.0    6.0    8.0  b'Q'  \n",
      "\n",
      "[20000 rows x 17 columns]\n"
     ]
    }
   ],
   "source": [
    "dataset, meta = arff.loadarff(folder_dataset)\n",
    "dataset = pd.DataFrame(dataset)\n",
    "dataset.head()\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribuição das classes por Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantas classes existem nesse dataset?\n",
      "26\n",
      "\n",
      "Quantas instâncias existem no dataset?\n",
      "20000\n",
      "\n",
      "Quantas features existem no dataset?\n",
      "16\n",
      "\n",
      "Que features são essas?\n",
      "['x-box', 'y-box', 'width', 'high', 'onpix', 'x-bar', 'y-bar', 'x2bar', 'y2bar', 'xybar', 'x2ybr', 'xy2br', 'x-ege', 'xegvy', 'y-ege', 'yegvx']\n",
      "\n",
      "Qual o numero de instâncias por classe?\n",
      "b'U'    813\n",
      "b'D'    805\n",
      "b'P'    803\n",
      "b'T'    796\n",
      "b'M'    792\n",
      "b'A'    789\n",
      "b'X'    787\n",
      "b'Y'    786\n",
      "b'Q'    783\n",
      "b'N'    783\n",
      "b'F'    775\n",
      "b'G'    773\n",
      "b'E'    768\n",
      "b'B'    766\n",
      "b'V'    764\n",
      "b'L'    761\n",
      "b'R'    758\n",
      "b'I'    755\n",
      "b'O'    753\n",
      "b'W'    752\n",
      "b'S'    748\n",
      "b'J'    747\n",
      "b'K'    739\n",
      "b'C'    736\n",
      "b'H'    734\n",
      "b'Z'    734\n",
      "Name: class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "target_col = 'class'\n",
    "print(\"Quantas classes existem nesse dataset?\\n%d\" %(len(dataset[target_col].unique())))\n",
    "print(\"\\nQuantas instâncias existem no dataset?\\n%d\" %(dataset.shape[0]))\n",
    "print(\"\\nQuantas features existem no dataset?\\n%d\" % (dataset.shape[1]-1))\n",
    "print(\"\\nQue features são essas?\\n%s\" % (str([k for k in dataset.keys() if k != target_col])))\n",
    "print(\"\\nQual o numero de instâncias por classe?\")\n",
    "print(dataset[target_col].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'U'    813\n",
      "b'D'    805\n",
      "b'P'    803\n",
      "b'T'    796\n",
      "b'M'    792\n",
      "b'A'    789\n",
      "b'X'    787\n",
      "b'Y'    786\n",
      "b'Q'    783\n",
      "b'N'    783\n",
      "b'F'    775\n",
      "b'G'    773\n",
      "b'E'    768\n",
      "b'B'    766\n",
      "b'V'    764\n",
      "b'L'    761\n",
      "b'R'    758\n",
      "b'I'    755\n",
      "b'O'    753\n",
      "b'W'    752\n",
      "b'S'    748\n",
      "b'J'    747\n",
      "b'K'    739\n",
      "b'C'    736\n",
      "b'H'    734\n",
      "b'Z'    734\n",
      "Name: class, dtype: int64\n",
      "20    813\n",
      "8     805\n",
      "1     803\n",
      "21    796\n",
      "7     792\n",
      "10    789\n",
      "16    787\n",
      "17    786\n",
      "14    783\n",
      "5     783\n",
      "4     775\n",
      "23    773\n",
      "12    768\n",
      "24    766\n",
      "9     764\n",
      "15    761\n",
      "6     758\n",
      "18    755\n",
      "13    753\n",
      "19    752\n",
      "2     748\n",
      "25    747\n",
      "11    739\n",
      "22    736\n",
      "3     734\n",
      "0     734\n",
      "Name: class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "target_col = 'class'\n",
    "s_dataset[target_col] = pd.factorize(s_dataset[target_col])[0]\n",
    "print(dataset[target_col].value_counts())\n",
    "print(s_dataset[target_col].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análise do dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2133058446828215e-15\n",
      "2.3514523661560816e-13\n"
     ]
    }
   ],
   "source": [
    "#Arrays do Banco de dados\n",
    "X = dataset.loc[:,\"x-box\":\"yegvx\"]\n",
    "y = dataset.loc[:,[target_col]]\n",
    "\n",
    "#Criando Dataset NORMALIZADO Z\n",
    "s_dataset = dataset.copy(deep= True)\n",
    "stdScaler = StandardScaler()\n",
    "stdScaler.fit(X)\n",
    "s_dataset.loc[:,\"x-box\":\"yegvx\"] = stdScaler.fit_transform(X,y=True)\n",
    "\n",
    "#Analisando médias e desvio padrão do dataset não normalizado e normalizado\n",
    "print (s_dataset.mean().max() - s_dataset.mean().min())\n",
    "print (s_dataset.std().max() - s_dataset.std().min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SETUP INICIAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([b'Z', b'P', b'S', b'H', b'F', b'N', b'R', b'M', b'D', b'V', b'A', b'K',\n",
      "       b'E', b'O', b'Q', b'L', b'X', b'Y', b'I', b'W', b'U', b'T', b'C', b'G',\n",
      "       b'B', b'J'],\n",
      "      dtype='object')\n",
      "20    813\n",
      "8     805\n",
      "1     803\n",
      "21    796\n",
      "7     792\n",
      "10    789\n",
      "16    787\n",
      "17    786\n",
      "14    783\n",
      "5     783\n",
      "4     775\n",
      "23    773\n",
      "12    768\n",
      "24    766\n",
      "9     764\n",
      "15    761\n",
      "6     758\n",
      "18    755\n",
      "13    753\n",
      "19    752\n",
      "2     748\n",
      "25    747\n",
      "11    739\n",
      "22    736\n",
      "3     734\n",
      "0     734\n",
      "Name: class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Refatorando varíaveis de saída do dataset padrão\n",
    "dataset[target_col], classArray = pd.factorize(dataset[target_col])\n",
    "print(classArray)\n",
    "\n",
    "#Refatorando varíaveis de saída do dataset normalizado\n",
    "s_dataset[target_col] = pd.factorize(s_dataset[target_col])[0]\n",
    "\n",
    "#Verificando features\n",
    "s_X = s_dataset.loc[:,\"x-box\":\"yegvx\"]\n",
    "s_y = s_dataset.loc[:,[target_col]]\n",
    "print(s_dataset[target_col].value_counts())\n",
    "\n",
    "#Gerando objeto kfold\n",
    "kFoldObj = KFold(n_splits=10,shuffle=True,random_state=169)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oversampling o banco de dados normalizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de atributos (features):  15\n",
      "(21138, 16)\n"
     ]
    }
   ],
   "source": [
    "smt = SMOTE(random_state=169)\n",
    "X_smt, y_smt = smt.fit_resample(s_X,s_y)\n",
    "\n",
    "print(\"Número de atributos (features): \", X_smt.drop([X_smt.columns[0]], axis = 1).shape[1])\n",
    "print(X_smt.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection (Extração de Caracteristicas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pca = PCA(n_components=5)\n",
    "X_pca = pca.fit_transform(X_smt)\n",
    "\n",
    "#Analisando os Vetores PCA\n",
    "plt.bar(\n",
    "    range(1,len(pca.explained_variance_)+1),\n",
    "    pca.explained_variance_\n",
    "    )\n",
    "\n",
    "plt.xlabel('PCA Feature')\n",
    "plt.ylabel('Explained variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#LDA\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "X_lda = lda.fit_transform(X_smt,y_smt)\n",
    "\n",
    "plt.bar(\n",
    "    range(1,len(lda.explained_variance_ratio_)+1),\n",
    "    lda.explained_variance_ratio_\n",
    "    )\n",
    "\n",
    "plt.xlabel('LDA Feature')\n",
    "plt.ylabel('Explained variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANOVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        f valor       p valor  class\n",
      "10  1304.820141  0.000000e+00      9\n",
      "6   1236.208625  0.000000e+00      5\n",
      "12  1143.589088  0.000000e+00     11\n",
      "13   971.671816  0.000000e+00      8\n",
      "11   967.041735  0.000000e+00     10\n",
      "8    794.212217  0.000000e+00      7\n",
      "14   679.016094  0.000000e+00      2\n",
      "7    428.336500  0.000000e+00      6\n",
      "5    373.760911  0.000000e+00      4\n",
      "9    368.922218  0.000000e+00      8\n",
      "15   299.643118  0.000000e+00      3\n",
      "2    139.630300  0.000000e+00      2\n",
      "4    117.110665  0.000000e+00      3\n",
      "0     81.206347  0.000000e+00      0\n",
      "3     10.952558  2.154672e-43      3\n",
      "1      3.516696  6.560515e-09      1\n"
     ]
    }
   ],
   "source": [
    "fs = SelectKBest(score_func=f_classif, k=16)\n",
    "fs.fit(X_smt, y_smt)\n",
    "X_train_fs = fs.fit_transform(X_smt,y_smt)\n",
    "\n",
    "fs_table = pd.DataFrame()\n",
    "\n",
    "fs_table[\"f valor\"] = fs.scores_\n",
    "fs_table[\"p valor\"] = fs.pvalues_\n",
    "fs_table[target_col] = y_smt[target_col]\n",
    "\n",
    "\n",
    "\n",
    "print(fs_table.sort_values([\"f valor\", \"p valor\"], ascending=[False, False]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HIPERPARÂMETROS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definindo distância Customizada para KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manhattan_custom_distance(x, y, weights=np.array([2, 1])):\n",
    "    return (abs(x - y)*weights).sum()\n",
    "\n",
    "#Arrays Hiperparametros\n",
    "ks = [1]\n",
    "distArray = [\"manhattan\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Para MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = [100]\n",
    "learningRateInit = [0.0001]\n",
    "learningRate= ['constant']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinamento do modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TREINAMENTOS KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Melhor parametro: {'metric': 'manhattan', 'n_neighbors': 1}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.90      0.84        84\n",
      "           1       0.66      0.70      0.68        71\n",
      "           2       0.72      0.60      0.65        93\n",
      "           3       0.72      0.60      0.66        91\n",
      "           4       0.75      0.67      0.71        76\n",
      "           5       0.82      0.85      0.83        86\n",
      "           6       0.53      0.59      0.56        80\n",
      "           7       0.89      0.94      0.92        69\n",
      "           8       0.56      0.53      0.54        92\n",
      "           9       0.85      0.93      0.88        95\n",
      "          10       0.86      0.87      0.87        87\n",
      "          11       0.66      0.75      0.70        77\n",
      "          12       0.70      0.70      0.70        73\n",
      "          13       0.60      0.62      0.61        85\n",
      "          14       0.76      0.59      0.67        96\n",
      "          15       0.85      0.87      0.86        94\n",
      "          16       0.73      0.70      0.72        70\n",
      "          17       0.68      0.74      0.71        89\n",
      "          18       0.87      0.88      0.87        81\n",
      "          19       0.91      0.86      0.89        72\n",
      "          20       0.76      0.80      0.78        76\n",
      "          21       0.79      0.73      0.76        79\n",
      "          22       0.89      0.86      0.87        84\n",
      "          23       0.70      0.75      0.72        63\n",
      "          24       0.60      0.64      0.62        77\n",
      "          25       0.83      0.81      0.82        74\n",
      "\n",
      "    accuracy                           0.75      2114\n",
      "   macro avg       0.75      0.75      0.75      2114\n",
      "weighted avg       0.75      0.75      0.75      2114\n",
      "\n",
      "Fold 0: 0.750\n",
      "Media do modelo  0.7495228055244388\n",
      "\n",
      "Melhor parametro: {'metric': 'manhattan', 'n_neighbors': 1}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.83      0.85        76\n",
      "           1       0.69      0.75      0.72        79\n",
      "           2       0.78      0.66      0.71        85\n",
      "           3       0.63      0.62      0.62        76\n",
      "           4       0.66      0.67      0.67        83\n",
      "           5       0.80      0.78      0.79        89\n",
      "           6       0.53      0.63      0.57        83\n",
      "           7       0.91      0.88      0.90        85\n",
      "           8       0.62      0.54      0.58        92\n",
      "           9       0.83      0.83      0.83        78\n",
      "          10       0.88      0.81      0.84        72\n",
      "          11       0.64      0.65      0.65        72\n",
      "          12       0.72      0.70      0.71        90\n",
      "          13       0.63      0.61      0.62        94\n",
      "          14       0.54      0.62      0.58        65\n",
      "          15       0.87      0.81      0.84        83\n",
      "          16       0.66      0.79      0.72        80\n",
      "          17       0.64      0.66      0.65        71\n",
      "          18       0.81      0.81      0.81        81\n",
      "          19       0.85      0.89      0.87        74\n",
      "          20       0.77      0.79      0.78        85\n",
      "          21       0.78      0.84      0.81        90\n",
      "          22       0.79      0.77      0.78        82\n",
      "          23       0.73      0.64      0.68        85\n",
      "          24       0.62      0.64      0.63        84\n",
      "          25       0.85      0.84      0.84        80\n",
      "\n",
      "    accuracy                           0.73      2114\n",
      "   macro avg       0.74      0.73      0.73      2114\n",
      "weighted avg       0.74      0.73      0.73      2114\n",
      "\n",
      "Fold 1: 0.735\n",
      "Media do modelo  0.735256744362281\n",
      "\n",
      "Melhor parametro: {'metric': 'manhattan', 'n_neighbors': 1}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.85      0.84        67\n",
      "           1       0.77      0.76      0.76        87\n",
      "           2       0.77      0.72      0.74        64\n",
      "           3       0.66      0.69      0.67        80\n",
      "           4       0.74      0.74      0.74        82\n",
      "           5       0.80      0.81      0.80        73\n",
      "           6       0.55      0.74      0.63        73\n",
      "           7       0.88      0.86      0.87        79\n",
      "           8       0.55      0.56      0.56        78\n",
      "           9       0.84      0.82      0.83        82\n",
      "          10       0.84      0.83      0.83        82\n",
      "          11       0.73      0.71      0.72       100\n",
      "          12       0.73      0.65      0.69       104\n",
      "          13       0.67      0.66      0.67        91\n",
      "          14       0.60      0.60      0.60        84\n",
      "          15       0.82      0.76      0.79        79\n",
      "          16       0.69      0.77      0.73        79\n",
      "          17       0.66      0.60      0.63        73\n",
      "          18       0.91      0.83      0.87        90\n",
      "          19       0.85      0.90      0.87        89\n",
      "          20       0.76      0.74      0.75        85\n",
      "          21       0.74      0.85      0.79        72\n",
      "          22       0.86      0.82      0.84        77\n",
      "          23       0.72      0.75      0.73        71\n",
      "          24       0.71      0.69      0.70        84\n",
      "          25       0.92      0.87      0.89        89\n",
      "\n",
      "    accuracy                           0.75      2114\n",
      "   macro avg       0.75      0.75      0.75      2114\n",
      "weighted avg       0.76      0.75      0.75      2114\n",
      "\n",
      "Fold 2: 0.754\n",
      "Media do modelo  0.7541755402208317\n",
      "\n",
      "Melhor parametro: {'metric': 'manhattan', 'n_neighbors': 1}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.91      0.89        64\n",
      "           1       0.75      0.78      0.77        97\n",
      "           2       0.74      0.64      0.69        76\n",
      "           3       0.69      0.77      0.72        77\n",
      "           4       0.67      0.65      0.66        89\n",
      "           5       0.81      0.80      0.80        79\n",
      "           6       0.56      0.58      0.57        85\n",
      "           7       0.87      0.82      0.85        74\n",
      "           8       0.60      0.60      0.60        83\n",
      "           9       0.86      0.71      0.78        86\n",
      "          10       0.88      0.90      0.89        87\n",
      "          11       0.57      0.50      0.53        74\n",
      "          12       0.66      0.63      0.64        73\n",
      "          13       0.65      0.66      0.66        77\n",
      "          14       0.69      0.63      0.66        91\n",
      "          15       0.80      0.86      0.83        79\n",
      "          16       0.65      0.78      0.71        74\n",
      "          17       0.72      0.70      0.71        93\n",
      "          18       0.88      0.90      0.89        71\n",
      "          19       0.92      0.89      0.91        81\n",
      "          20       0.81      0.86      0.83        85\n",
      "          21       0.75      0.81      0.78        79\n",
      "          22       0.86      0.81      0.83        90\n",
      "          23       0.75      0.74      0.75       101\n",
      "          24       0.60      0.64      0.62        75\n",
      "          25       0.83      0.88      0.86        74\n",
      "\n",
      "    accuracy                           0.75      2114\n",
      "   macro avg       0.75      0.75      0.75      2114\n",
      "weighted avg       0.75      0.75      0.75      2114\n",
      "\n",
      "Fold 3: 0.748\n",
      "Media do modelo  0.7479388508452919\n",
      "\n",
      "Melhor parametro: {'metric': 'manhattan', 'n_neighbors': 1}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.81      0.84        94\n",
      "           1       0.68      0.76      0.72        78\n",
      "           2       0.70      0.67      0.69        88\n",
      "           3       0.70      0.73      0.71        82\n",
      "           4       0.79      0.71      0.75        86\n",
      "           5       0.77      0.78      0.78        78\n",
      "           6       0.46      0.54      0.50        74\n",
      "           7       0.86      0.83      0.84        78\n",
      "           8       0.56      0.62      0.59        92\n",
      "           9       0.91      0.81      0.86        78\n",
      "          10       0.87      0.83      0.85        82\n",
      "          11       0.76      0.76      0.76        86\n",
      "          12       0.69      0.68      0.68        68\n",
      "          13       0.65      0.68      0.67        73\n",
      "          14       0.63      0.64      0.63        69\n",
      "          15       0.79      0.83      0.81        77\n",
      "          16       0.73      0.82      0.77        82\n",
      "          17       0.76      0.72      0.74        86\n",
      "          18       0.86      0.83      0.84        86\n",
      "          19       0.92      0.94      0.93        87\n",
      "          20       0.71      0.79      0.75        70\n",
      "          21       0.78      0.68      0.72        74\n",
      "          22       0.84      0.76      0.80        88\n",
      "          23       0.79      0.76      0.78        80\n",
      "          24       0.61      0.57      0.59        87\n",
      "          25       0.81      0.86      0.83        91\n",
      "\n",
      "    accuracy                           0.75      2114\n",
      "   macro avg       0.75      0.75      0.75      2114\n",
      "weighted avg       0.75      0.75      0.75      2114\n",
      "\n",
      "Fold 4: 0.750\n",
      "Media do modelo  0.7499777671816882\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Melhor parametro: {'metric': 'manhattan', 'n_neighbors': 1}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.83      0.84        82\n",
      "           1       0.74      0.68      0.71        73\n",
      "           2       0.74      0.71      0.72        76\n",
      "           3       0.66      0.82      0.73        71\n",
      "           4       0.67      0.83      0.74        70\n",
      "           5       0.84      0.81      0.83        85\n",
      "           6       0.68      0.64      0.66        92\n",
      "           7       0.86      0.90      0.88        90\n",
      "           8       0.46      0.50      0.48        72\n",
      "           9       0.85      0.84      0.84        80\n",
      "          10       0.85      0.89      0.87        84\n",
      "          11       0.73      0.61      0.66       104\n",
      "          12       0.74      0.68      0.71        85\n",
      "          13       0.61      0.56      0.59        94\n",
      "          14       0.62      0.63      0.62        67\n",
      "          15       0.82      0.84      0.83        83\n",
      "          16       0.71      0.82      0.76        71\n",
      "          17       0.79      0.77      0.78        71\n",
      "          18       0.83      0.87      0.85        78\n",
      "          19       0.93      0.94      0.94        89\n",
      "          20       0.75      0.77      0.76        90\n",
      "          21       0.88      0.79      0.83        89\n",
      "          22       0.84      0.81      0.82        78\n",
      "          23       0.71      0.64      0.67        86\n",
      "          24       0.60      0.58      0.59        86\n",
      "          25       0.78      0.82      0.80        68\n",
      "\n",
      "    accuracy                           0.75      2114\n",
      "   macro avg       0.75      0.75      0.75      2114\n",
      "weighted avg       0.75      0.75      0.75      2114\n",
      "\n",
      "Fold 5: 0.751\n",
      "Media do modelo  0.7505181229738489\n",
      "\n",
      "Melhor parametro: {'metric': 'manhattan', 'n_neighbors': 1}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.90      0.86        78\n",
      "           1       0.72      0.67      0.69        72\n",
      "           2       0.73      0.73      0.73        82\n",
      "           3       0.72      0.69      0.70        86\n",
      "           4       0.76      0.75      0.75        83\n",
      "           5       0.80      0.75      0.78        88\n",
      "           6       0.59      0.65      0.61        79\n",
      "           7       0.86      0.88      0.87        86\n",
      "           8       0.53      0.61      0.57        66\n",
      "           9       0.90      0.82      0.86        87\n",
      "          10       0.93      0.86      0.89       103\n",
      "          11       0.66      0.58      0.62        93\n",
      "          12       0.72      0.79      0.76        68\n",
      "          13       0.51      0.60      0.55        73\n",
      "          14       0.58      0.64      0.61        70\n",
      "          15       0.90      0.90      0.90        86\n",
      "          16       0.79      0.67      0.72        99\n",
      "          17       0.73      0.68      0.71        76\n",
      "          18       0.82      0.85      0.84        89\n",
      "          19       0.84      0.95      0.89        80\n",
      "          20       0.75      0.72      0.74        72\n",
      "          21       0.83      0.80      0.81        69\n",
      "          22       0.80      0.79      0.79        85\n",
      "          23       0.80      0.72      0.75        92\n",
      "          24       0.57      0.62      0.60        69\n",
      "          25       0.82      0.87      0.84        83\n",
      "\n",
      "    accuracy                           0.75      2114\n",
      "   macro avg       0.75      0.75      0.75      2114\n",
      "weighted avg       0.76      0.75      0.75      2114\n",
      "\n",
      "Fold 6: 0.749\n",
      "Media do modelo  0.7494002199028197\n",
      "\n",
      "Melhor parametro: {'metric': 'manhattan', 'n_neighbors': 1}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.80      0.84        90\n",
      "           1       0.76      0.69      0.72        84\n",
      "           2       0.63      0.77      0.69        78\n",
      "           3       0.69      0.65      0.67        94\n",
      "           4       0.78      0.75      0.77        76\n",
      "           5       0.77      0.85      0.80        78\n",
      "           6       0.60      0.60      0.60        82\n",
      "           7       0.94      0.92      0.93        79\n",
      "           8       0.55      0.62      0.59        74\n",
      "           9       0.83      0.91      0.87        68\n",
      "          10       0.77      0.85      0.81        65\n",
      "          11       0.73      0.80      0.77        76\n",
      "          12       0.77      0.70      0.73        88\n",
      "          13       0.53      0.71      0.61        58\n",
      "          14       0.67      0.65      0.66        81\n",
      "          15       0.89      0.84      0.86        83\n",
      "          16       0.81      0.83      0.82        94\n",
      "          17       0.70      0.68      0.69        84\n",
      "          18       0.86      0.84      0.85        88\n",
      "          19       0.91      0.91      0.91        80\n",
      "          20       0.80      0.73      0.76        97\n",
      "          21       0.87      0.77      0.82        87\n",
      "          22       0.80      0.83      0.82        90\n",
      "          23       0.74      0.70      0.72        80\n",
      "          24       0.82      0.68      0.75        92\n",
      "          25       0.86      0.88      0.87        68\n",
      "\n",
      "    accuracy                           0.77      2114\n",
      "   macro avg       0.77      0.77      0.77      2114\n",
      "weighted avg       0.77      0.77      0.77      2114\n",
      "\n",
      "Fold 7: 0.768\n",
      "Media do modelo  0.7678140244754796\n",
      "\n",
      "Melhor parametro: {'metric': 'manhattan', 'n_neighbors': 1}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.83      0.83        81\n",
      "           1       0.75      0.75      0.75        96\n",
      "           2       0.77      0.71      0.74        87\n",
      "           3       0.67      0.62      0.64        68\n",
      "           4       0.71      0.74      0.73        93\n",
      "           5       0.84      0.82      0.83        88\n",
      "           6       0.56      0.62      0.59        72\n",
      "           7       0.88      0.90      0.89        89\n",
      "           8       0.60      0.62      0.61        88\n",
      "           9       0.83      0.85      0.84        87\n",
      "          10       0.81      0.79      0.80        70\n",
      "          11       0.57      0.71      0.63        59\n",
      "          12       0.71      0.75      0.73        76\n",
      "          13       0.67      0.75      0.71        71\n",
      "          14       0.71      0.66      0.69       106\n",
      "          15       0.81      0.85      0.83        72\n",
      "          16       0.78      0.79      0.78        75\n",
      "          17       0.72      0.74      0.73        77\n",
      "          18       0.89      0.88      0.88        73\n",
      "          19       0.91      0.86      0.89        74\n",
      "          20       0.73      0.72      0.73        79\n",
      "          21       0.75      0.70      0.73        83\n",
      "          22       0.81      0.74      0.78        82\n",
      "          23       0.67      0.76      0.71        79\n",
      "          24       0.70      0.63      0.67        90\n",
      "          25       0.87      0.76      0.81        98\n",
      "\n",
      "    accuracy                           0.75      2113\n",
      "   macro avg       0.75      0.75      0.75      2113\n",
      "weighted avg       0.75      0.75      0.75      2113\n",
      "\n",
      "Fold 8: 0.752\n",
      "Media do modelo  0.7521529278390922\n",
      "\n",
      "Melhor parametro: {'metric': 'manhattan', 'n_neighbors': 1}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.82      0.84        97\n",
      "           1       0.70      0.75      0.73        76\n",
      "           2       0.64      0.71      0.67        84\n",
      "           3       0.67      0.67      0.67        88\n",
      "           4       0.68      0.69      0.68        75\n",
      "           5       0.78      0.77      0.77        69\n",
      "           6       0.65      0.66      0.65        93\n",
      "           7       0.90      0.87      0.88        84\n",
      "           8       0.56      0.61      0.58        76\n",
      "           9       0.84      0.82      0.83        72\n",
      "          10       0.80      0.90      0.85        81\n",
      "          11       0.61      0.60      0.60        72\n",
      "          12       0.65      0.60      0.63        88\n",
      "          13       0.67      0.65      0.66        97\n",
      "          14       0.66      0.61      0.63        84\n",
      "          15       0.75      0.79      0.77        77\n",
      "          16       0.70      0.65      0.67        89\n",
      "          17       0.81      0.68      0.74        93\n",
      "          18       0.73      0.83      0.78        76\n",
      "          19       0.88      0.92      0.90        87\n",
      "          20       0.75      0.77      0.76        74\n",
      "          21       0.81      0.86      0.83        91\n",
      "          22       0.82      0.70      0.75        57\n",
      "          23       0.66      0.72      0.69        76\n",
      "          24       0.64      0.61      0.62        69\n",
      "          25       0.89      0.82      0.85        88\n",
      "\n",
      "    accuracy                           0.73      2113\n",
      "   macro avg       0.73      0.73      0.73      2113\n",
      "weighted avg       0.74      0.73      0.73      2113\n",
      "\n",
      "Fold 9: 0.735\n",
      "Media do modelo  0.7348055956963961\n",
      "\n",
      "Precisão média (desvio): 0.749 +- (0.009)\n"
     ]
    }
   ],
   "source": [
    "model = KNeighborsClassifier()\n",
    "parameters = {'n_neighbors': ks,\n",
    "                      'metric':distArray}\n",
    "\n",
    "grid = GridSearchCV(estimator = model,\n",
    "                    param_grid = parameters,\n",
    "                    scoring = 'precision_macro',\n",
    "                    cv = 3)\n",
    "def evaluate_model_with_kfold(kf):\n",
    "    precision_list = []\n",
    "    fold = 0\n",
    "    for train, test in kf.split(X_smt, y_smt):\n",
    "\n",
    "        y_train, y_test = y_smt.iloc[train], y_smt.iloc[test]\n",
    "\n",
    "        X_pca_train = pca.fit_transform(X_smt.iloc[train])\n",
    "        X_pca_test = pca.transform(X_smt.iloc[test])\n",
    "\n",
    "        grid.fit(X_pca_train, y_train)\n",
    "\n",
    "        y_pred = grid.predict(X_pca_test)\n",
    "\n",
    "        print(\"\\nMelhor parametro:\", grid.best_params_)\n",
    "        print(classification_report(y_test, grid.predict(X_pca_test)))\n",
    "        print(\"Fold %d: %.3f\" %(fold, precision_score(y_test, y_pred,average=\"macro\")))\n",
    "\n",
    "        precision_list.append(precision_score(y_test, y_pred,average=\"macro\"))\n",
    "        print(\"Media do modelo \", precision_list[fold].mean())\n",
    "\n",
    "        fold += 1\n",
    "\n",
    "    precision = np.array(precision_list)\n",
    "    print(\"\\nPrecisão média (desvio): %.3f +- (%.3f)\" %(precision.mean(), precision.std()))\n",
    "\n",
    "# chamada da função de Treino com KFOLD\n",
    "evaluate_model_with_kfold(kFoldObj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Melhor parametro: {'metric': 'manhattan', 'n_neighbors': 1}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99        84\n",
      "           1       0.97      1.00      0.99        71\n",
      "           2       1.00      0.96      0.98        93\n",
      "           3       0.87      0.86      0.86        91\n",
      "           4       0.99      0.96      0.97        76\n",
      "           5       0.97      0.97      0.97        86\n",
      "           6       0.94      0.93      0.93        80\n",
      "           7       0.97      0.99      0.98        69\n",
      "           8       0.95      0.97      0.96        92\n",
      "           9       0.96      0.98      0.97        95\n",
      "          10       1.00      0.99      0.99        87\n",
      "          11       0.89      0.94      0.91        77\n",
      "          12       0.95      0.96      0.95        73\n",
      "          13       0.95      0.96      0.96        85\n",
      "          14       0.99      0.95      0.97        96\n",
      "          15       0.96      0.99      0.97        94\n",
      "          16       0.97      0.96      0.96        70\n",
      "          17       0.99      0.99      0.99        89\n",
      "          18       0.99      0.96      0.97        81\n",
      "          19       0.99      0.99      0.99        72\n",
      "          20       0.99      0.95      0.97        76\n",
      "          21       0.94      1.00      0.97        79\n",
      "          22       0.99      1.00      0.99        84\n",
      "          23       0.97      0.97      0.97        63\n",
      "          24       0.97      0.92      0.95        77\n",
      "          25       0.97      0.97      0.97        74\n",
      "\n",
      "    accuracy                           0.96      2114\n",
      "   macro avg       0.97      0.96      0.96      2114\n",
      "weighted avg       0.96      0.96      0.96      2114\n",
      "\n",
      "Fold 0: 0.965\n",
      "Media do modelo  0.9650425684680605\n",
      "\n",
      "Melhor parametro: {'metric': 'manhattan', 'n_neighbors': 1}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99        76\n",
      "           1       0.99      0.92      0.95        79\n",
      "           2       0.98      0.95      0.96        85\n",
      "           3       0.86      0.89      0.88        76\n",
      "           4       0.96      0.95      0.96        83\n",
      "           5       0.98      0.94      0.96        89\n",
      "           6       0.94      0.92      0.93        83\n",
      "           7       0.98      0.98      0.98        85\n",
      "           8       0.92      0.95      0.93        92\n",
      "           9       0.96      0.97      0.97        78\n",
      "          10       1.00      1.00      1.00        72\n",
      "          11       0.89      0.89      0.89        72\n",
      "          12       0.95      0.92      0.94        90\n",
      "          13       0.97      0.93      0.95        94\n",
      "          14       0.97      0.97      0.97        65\n",
      "          15       0.96      0.96      0.96        83\n",
      "          16       0.95      0.95      0.95        80\n",
      "          17       0.96      0.96      0.96        71\n",
      "          18       1.00      0.98      0.99        81\n",
      "          19       0.97      0.99      0.98        74\n",
      "          20       0.97      0.99      0.98        85\n",
      "          21       0.96      0.99      0.97        90\n",
      "          22       0.99      0.99      0.99        82\n",
      "          23       0.94      0.94      0.94        85\n",
      "          24       0.90      0.98      0.94        84\n",
      "          25       0.98      0.99      0.98        80\n",
      "\n",
      "    accuracy                           0.96      2114\n",
      "   macro avg       0.96      0.96      0.96      2114\n",
      "weighted avg       0.96      0.96      0.96      2114\n",
      "\n",
      "Fold 1: 0.957\n",
      "Media do modelo  0.9574814735662412\n",
      "\n",
      "Melhor parametro: {'metric': 'manhattan', 'n_neighbors': 1}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99        67\n",
      "           1       0.95      0.92      0.94        87\n",
      "           2       0.97      1.00      0.98        64\n",
      "           3       0.87      0.91      0.89        80\n",
      "           4       0.94      0.93      0.93        82\n",
      "           5       0.96      0.97      0.97        73\n",
      "           6       0.93      0.88      0.90        73\n",
      "           7       1.00      0.99      0.99        79\n",
      "           8       0.91      0.88      0.90        78\n",
      "           9       0.98      1.00      0.99        82\n",
      "          10       0.98      0.99      0.98        82\n",
      "          11       0.94      0.90      0.92       100\n",
      "          12       0.96      0.93      0.95       104\n",
      "          13       0.93      0.98      0.95        91\n",
      "          14       0.99      0.95      0.97        84\n",
      "          15       1.00      0.97      0.99        79\n",
      "          16       0.94      0.95      0.94        79\n",
      "          17       0.97      0.96      0.97        73\n",
      "          18       0.98      0.96      0.97        90\n",
      "          19       1.00      1.00      1.00        89\n",
      "          20       0.97      0.98      0.97        85\n",
      "          21       0.97      0.97      0.97        72\n",
      "          22       0.96      0.97      0.97        77\n",
      "          23       0.95      0.97      0.96        71\n",
      "          24       0.92      0.95      0.94        84\n",
      "          25       0.97      0.99      0.98        89\n",
      "\n",
      "    accuracy                           0.96      2114\n",
      "   macro avg       0.96      0.96      0.96      2114\n",
      "weighted avg       0.96      0.96      0.96      2114\n",
      "\n",
      "Fold 2: 0.957\n",
      "Media do modelo  0.9573830794294319\n",
      "\n",
      "Melhor parametro: {'metric': 'manhattan', 'n_neighbors': 1}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        64\n",
      "           1       0.96      0.95      0.95        97\n",
      "           2       0.97      0.96      0.97        76\n",
      "           3       0.91      0.88      0.89        77\n",
      "           4       0.88      0.91      0.90        89\n",
      "           5       0.99      0.94      0.96        79\n",
      "           6       0.93      0.94      0.94        85\n",
      "           7       1.00      0.99      0.99        74\n",
      "           8       0.89      0.96      0.92        83\n",
      "           9       0.95      0.91      0.93        86\n",
      "          10       1.00      0.99      0.99        87\n",
      "          11       0.94      0.82      0.88        74\n",
      "          12       0.93      0.92      0.92        73\n",
      "          13       0.89      0.95      0.92        77\n",
      "          14       0.98      0.95      0.96        91\n",
      "          15       0.96      1.00      0.98        79\n",
      "          16       0.90      0.97      0.94        74\n",
      "          17       0.98      0.98      0.98        93\n",
      "          18       0.97      0.97      0.97        71\n",
      "          19       0.99      0.98      0.98        81\n",
      "          20       0.99      1.00      0.99        85\n",
      "          21       1.00      0.95      0.97        79\n",
      "          22       0.97      0.98      0.97        90\n",
      "          23       0.91      0.95      0.93       101\n",
      "          24       0.91      0.95      0.93        75\n",
      "          25       1.00      0.97      0.99        74\n",
      "\n",
      "    accuracy                           0.95      2114\n",
      "   macro avg       0.95      0.95      0.95      2114\n",
      "weighted avg       0.95      0.95      0.95      2114\n",
      "\n",
      "Fold 3: 0.954\n",
      "Media do modelo  0.9535960343798917\n",
      "\n",
      "Melhor parametro: {'metric': 'manhattan', 'n_neighbors': 1}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        94\n",
      "           1       0.96      0.87      0.91        78\n",
      "           2       1.00      0.99      0.99        88\n",
      "           3       0.90      0.94      0.92        82\n",
      "           4       0.92      0.97      0.94        86\n",
      "           5       0.96      0.97      0.97        78\n",
      "           6       0.91      0.95      0.93        74\n",
      "           7       0.97      0.97      0.97        78\n",
      "           8       0.98      0.92      0.95        92\n",
      "           9       0.97      0.97      0.97        78\n",
      "          10       1.00      0.99      0.99        82\n",
      "          11       0.95      0.92      0.93        86\n",
      "          12       0.96      0.96      0.96        68\n",
      "          13       0.96      0.97      0.97        73\n",
      "          14       0.97      0.96      0.96        69\n",
      "          15       0.96      0.99      0.97        77\n",
      "          16       0.95      0.99      0.97        82\n",
      "          17       1.00      0.98      0.99        86\n",
      "          18       0.99      0.99      0.99        86\n",
      "          19       0.99      0.98      0.98        87\n",
      "          20       0.95      1.00      0.97        70\n",
      "          21       0.95      0.97      0.96        74\n",
      "          22       0.98      1.00      0.99        88\n",
      "          23       0.97      0.95      0.96        80\n",
      "          24       0.94      0.92      0.93        87\n",
      "          25       0.99      0.98      0.98        91\n",
      "\n",
      "    accuracy                           0.96      2114\n",
      "   macro avg       0.96      0.96      0.96      2114\n",
      "weighted avg       0.97      0.96      0.96      2114\n",
      "\n",
      "Fold 4: 0.965\n",
      "Media do modelo  0.9645049092699177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Melhor parametro: {'metric': 'manhattan', 'n_neighbors': 1}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99        82\n",
      "           1       0.96      0.95      0.95        73\n",
      "           2       0.99      0.99      0.99        76\n",
      "           3       0.86      0.93      0.89        71\n",
      "           4       0.93      0.94      0.94        70\n",
      "           5       0.99      0.95      0.97        85\n",
      "           6       0.96      0.99      0.97        92\n",
      "           7       1.00      1.00      1.00        90\n",
      "           8       0.96      0.93      0.94        72\n",
      "           9       0.99      0.97      0.98        80\n",
      "          10       0.99      1.00      0.99        84\n",
      "          11       0.96      0.90      0.93       104\n",
      "          12       0.99      0.94      0.96        85\n",
      "          13       0.97      0.97      0.97        94\n",
      "          14       0.93      0.94      0.93        67\n",
      "          15       0.98      0.98      0.98        83\n",
      "          16       0.99      0.99      0.99        71\n",
      "          17       0.97      0.99      0.98        71\n",
      "          18       0.95      0.99      0.97        78\n",
      "          19       0.99      1.00      0.99        89\n",
      "          20       0.98      0.99      0.98        90\n",
      "          21       0.99      0.94      0.97        89\n",
      "          22       0.97      0.97      0.97        78\n",
      "          23       0.94      0.95      0.95        86\n",
      "          24       0.94      0.94      0.94        86\n",
      "          25       0.94      0.94      0.94        68\n",
      "\n",
      "    accuracy                           0.96      2114\n",
      "   macro avg       0.96      0.96      0.96      2114\n",
      "weighted avg       0.97      0.96      0.97      2114\n",
      "\n",
      "Fold 5: 0.964\n",
      "Media do modelo  0.9641395724772986\n",
      "\n",
      "Melhor parametro: {'metric': 'manhattan', 'n_neighbors': 1}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99        78\n",
      "           1       0.92      0.94      0.93        72\n",
      "           2       0.98      0.96      0.97        82\n",
      "           3       0.92      0.98      0.95        86\n",
      "           4       0.95      0.95      0.95        83\n",
      "           5       0.99      0.97      0.98        88\n",
      "           6       0.93      0.94      0.93        79\n",
      "           7       0.99      0.98      0.98        86\n",
      "           8       0.96      0.97      0.96        66\n",
      "           9       0.97      0.97      0.97        87\n",
      "          10       1.00      0.99      1.00       103\n",
      "          11       0.95      0.92      0.93        93\n",
      "          12       0.93      0.99      0.96        68\n",
      "          13       0.94      0.93      0.94        73\n",
      "          14       0.98      0.93      0.96        70\n",
      "          15       0.98      0.98      0.98        86\n",
      "          16       0.98      0.95      0.96        99\n",
      "          17       0.99      0.96      0.97        76\n",
      "          18       0.99      0.97      0.98        89\n",
      "          19       0.96      0.97      0.97        80\n",
      "          20       0.94      1.00      0.97        72\n",
      "          21       0.96      0.97      0.96        69\n",
      "          22       0.96      0.96      0.96        85\n",
      "          23       0.95      0.93      0.94        92\n",
      "          24       0.91      0.88      0.90        69\n",
      "          25       0.98      0.99      0.98        83\n",
      "\n",
      "    accuracy                           0.96      2114\n",
      "   macro avg       0.96      0.96      0.96      2114\n",
      "weighted avg       0.96      0.96      0.96      2114\n",
      "\n",
      "Fold 6: 0.960\n",
      "Media do modelo  0.9602376280903313\n",
      "\n",
      "Melhor parametro: {'metric': 'manhattan', 'n_neighbors': 1}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98        90\n",
      "           1       0.97      0.92      0.94        84\n",
      "           2       0.97      0.99      0.98        78\n",
      "           3       0.92      0.90      0.91        94\n",
      "           4       0.94      0.97      0.95        76\n",
      "           5       0.97      0.96      0.97        78\n",
      "           6       0.98      0.98      0.98        82\n",
      "           7       0.96      0.99      0.97        79\n",
      "           8       0.95      0.95      0.95        74\n",
      "           9       0.96      0.97      0.96        68\n",
      "          10       0.97      0.98      0.98        65\n",
      "          11       0.86      0.97      0.91        76\n",
      "          12       0.96      0.92      0.94        88\n",
      "          13       0.92      0.95      0.93        58\n",
      "          14       0.96      0.94      0.95        81\n",
      "          15       1.00      0.98      0.99        83\n",
      "          16       0.98      0.96      0.97        94\n",
      "          17       0.98      0.98      0.98        84\n",
      "          18       0.97      0.99      0.98        88\n",
      "          19       1.00      0.97      0.99        80\n",
      "          20       0.97      0.96      0.96        97\n",
      "          21       0.98      0.98      0.98        87\n",
      "          22       0.98      0.98      0.98        90\n",
      "          23       0.97      0.94      0.96        80\n",
      "          24       0.98      0.98      0.98        92\n",
      "          25       0.96      0.97      0.96        68\n",
      "\n",
      "    accuracy                           0.96      2114\n",
      "   macro avg       0.96      0.96      0.96      2114\n",
      "weighted avg       0.96      0.96      0.96      2114\n",
      "\n",
      "Fold 7: 0.963\n",
      "Media do modelo  0.9626801153688062\n",
      "\n",
      "Melhor parametro: {'metric': 'manhattan', 'n_neighbors': 1}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99        81\n",
      "           1       0.96      0.96      0.96        96\n",
      "           2       1.00      1.00      1.00        87\n",
      "           3       0.92      0.87      0.89        68\n",
      "           4       0.94      0.97      0.95        93\n",
      "           5       0.97      0.99      0.98        88\n",
      "           6       0.96      0.93      0.94        72\n",
      "           7       0.99      0.99      0.99        89\n",
      "           8       0.91      0.97      0.94        88\n",
      "           9       0.97      0.98      0.97        87\n",
      "          10       0.99      0.99      0.99        70\n",
      "          11       0.90      0.93      0.92        59\n",
      "          12       0.95      0.92      0.93        76\n",
      "          13       0.96      0.96      0.96        71\n",
      "          14       0.98      0.97      0.98       106\n",
      "          15       0.99      0.97      0.98        72\n",
      "          16       0.96      0.96      0.96        75\n",
      "          17       0.96      0.96      0.96        77\n",
      "          18       0.93      0.97      0.95        73\n",
      "          19       0.97      0.96      0.97        74\n",
      "          20       0.96      0.94      0.95        79\n",
      "          21       0.99      0.96      0.98        83\n",
      "          22       0.99      0.94      0.96        82\n",
      "          23       0.93      0.96      0.94        79\n",
      "          24       0.93      0.94      0.94        90\n",
      "          25       0.98      0.95      0.96        98\n",
      "\n",
      "    accuracy                           0.96      2113\n",
      "   macro avg       0.96      0.96      0.96      2113\n",
      "weighted avg       0.96      0.96      0.96      2113\n",
      "\n",
      "Fold 8: 0.960\n",
      "Media do modelo  0.959521330686527\n",
      "\n",
      "Melhor parametro: {'metric': 'manhattan', 'n_neighbors': 1}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.98        97\n",
      "           1       0.93      0.93      0.93        76\n",
      "           2       0.98      0.99      0.98        84\n",
      "           3       0.88      0.89      0.88        88\n",
      "           4       0.93      0.95      0.94        75\n",
      "           5       0.96      0.96      0.96        69\n",
      "           6       0.94      0.90      0.92        93\n",
      "           7       0.99      0.99      0.99        84\n",
      "           8       0.94      0.95      0.94        76\n",
      "           9       0.94      0.94      0.94        72\n",
      "          10       0.99      0.99      0.99        81\n",
      "          11       0.84      0.92      0.87        72\n",
      "          12       0.97      0.94      0.95        88\n",
      "          13       0.97      0.94      0.95        97\n",
      "          14       0.94      0.95      0.95        84\n",
      "          15       0.95      0.97      0.96        77\n",
      "          16       0.99      0.97      0.98        89\n",
      "          17       0.97      0.98      0.97        93\n",
      "          18       0.97      0.96      0.97        76\n",
      "          19       1.00      1.00      1.00        87\n",
      "          20       0.97      0.96      0.97        74\n",
      "          21       0.98      0.96      0.97        91\n",
      "          22       0.98      0.93      0.95        57\n",
      "          23       0.89      0.93      0.91        76\n",
      "          24       0.91      0.91      0.91        69\n",
      "          25       0.97      0.97      0.97        88\n",
      "\n",
      "    accuracy                           0.95      2113\n",
      "   macro avg       0.95      0.95      0.95      2113\n",
      "weighted avg       0.95      0.95      0.95      2113\n",
      "\n",
      "Fold 9: 0.952\n",
      "Media do modelo  0.9520637469521213\n",
      "\n",
      "Precisão média (desvio): 0.960 +- (0.004)\n"
     ]
    }
   ],
   "source": [
    "model = KNeighborsClassifier()\n",
    "parameters = {'n_neighbors': ks,\n",
    "                      'metric':distArray}\n",
    "\n",
    "grid = GridSearchCV(estimator = model,\n",
    "                    param_grid = parameters,\n",
    "                    scoring = 'precision_macro',\n",
    "                    cv = 3)\n",
    "def evaluate_model_with_kfold(kf):\n",
    "    precision_list = []\n",
    "    fold = 0\n",
    "    for train, test in kf.split(X_smt, y_smt):\n",
    "\n",
    "        y_train, y_test = y_smt.iloc[train], y_smt.iloc[test]\n",
    "\n",
    "        X_lda_train = lda.fit_transform(X_smt.iloc[train],y_train)\n",
    "        X_lda_test = lda.transform(X_smt.iloc[test])\n",
    "\n",
    "        grid.fit(X_lda_train, y_train)\n",
    "\n",
    "        y_pred = grid.predict(X_lda_test)\n",
    "\n",
    "        print(\"\\nMelhor parametro:\", grid.best_params_)\n",
    "        print(classification_report(y_test, grid.predict(X_lda_test)))\n",
    "        print(\"Fold %d: %.3f\" %(fold, precision_score(y_test, y_pred,average=\"macro\")))\n",
    "\n",
    "        precision_list.append(precision_score(y_test, y_pred,average=\"macro\"))\n",
    "        print(\"Media do modelo \", precision_list[fold].mean())\n",
    "\n",
    "        fold += 1\n",
    "\n",
    "    precision = np.array(precision_list)\n",
    "    print(\"\\nPrecisão média (desvio): %.3f +- (%.3f)\" %(precision.mean(), precision.std()))\n",
    "\n",
    "# chamada da função de Treino com KFOLD\n",
    "evaluate_model_with_kfold(kFoldObj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN - ANOVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Melhor parametro: {'metric': 'manhattan', 'n_neighbors': 1}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        84\n",
      "           1       0.94      0.94      0.94        71\n",
      "           2       0.99      0.98      0.98        93\n",
      "           3       0.89      0.80      0.84        91\n",
      "           4       0.93      0.92      0.93        76\n",
      "           5       0.98      0.95      0.96        86\n",
      "           6       0.90      0.89      0.89        80\n",
      "           7       0.97      0.97      0.97        69\n",
      "           8       0.91      0.93      0.92        92\n",
      "           9       0.97      0.99      0.98        95\n",
      "          10       1.00      1.00      1.00        87\n",
      "          11       0.91      0.97      0.94        77\n",
      "          12       0.97      0.95      0.96        73\n",
      "          13       0.94      0.96      0.95        85\n",
      "          14       0.99      0.97      0.98        96\n",
      "          15       0.97      0.98      0.97        94\n",
      "          16       0.99      0.97      0.98        70\n",
      "          17       0.93      0.99      0.96        89\n",
      "          18       0.97      0.93      0.95        81\n",
      "          19       0.99      1.00      0.99        72\n",
      "          20       1.00      0.97      0.99        76\n",
      "          21       0.97      0.92      0.95        79\n",
      "          22       0.99      0.98      0.98        84\n",
      "          23       0.94      0.98      0.96        63\n",
      "          24       0.88      0.91      0.89        77\n",
      "          25       0.91      0.97      0.94        74\n",
      "\n",
      "    accuracy                           0.96      2114\n",
      "   macro avg       0.96      0.96      0.95      2114\n",
      "weighted avg       0.96      0.96      0.95      2114\n",
      "\n",
      "Fold 0: 0.955\n",
      "Media do modelo  0.9550874253047046\n",
      "\n",
      "Melhor parametro: {'metric': 'manhattan', 'n_neighbors': 1}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97        76\n",
      "           1       1.00      0.94      0.97        79\n",
      "           2       0.98      0.98      0.98        85\n",
      "           3       0.84      0.87      0.85        76\n",
      "           4       0.95      0.93      0.94        83\n",
      "           5       0.99      0.93      0.96        89\n",
      "           6       0.91      0.87      0.89        83\n",
      "           7       0.96      0.96      0.96        85\n",
      "           8       0.92      0.99      0.95        92\n",
      "           9       0.88      0.96      0.92        78\n",
      "          10       1.00      0.97      0.99        72\n",
      "          11       0.93      0.88      0.90        72\n",
      "          12       0.89      0.93      0.91        90\n",
      "          13       0.94      0.94      0.94        94\n",
      "          14       0.91      0.94      0.92        65\n",
      "          15       0.99      0.98      0.98        83\n",
      "          16       0.93      0.96      0.94        80\n",
      "          17       0.99      0.96      0.97        71\n",
      "          18       0.98      0.98      0.98        81\n",
      "          19       0.93      0.96      0.95        74\n",
      "          20       0.99      0.96      0.98        85\n",
      "          21       0.97      0.99      0.98        90\n",
      "          22       0.99      0.95      0.97        82\n",
      "          23       0.94      0.93      0.93        85\n",
      "          24       0.92      0.93      0.92        84\n",
      "          25       0.97      0.97      0.97        80\n",
      "\n",
      "    accuracy                           0.95      2114\n",
      "   macro avg       0.95      0.95      0.95      2114\n",
      "weighted avg       0.95      0.95      0.95      2114\n",
      "\n",
      "Fold 1: 0.948\n",
      "Media do modelo  0.9482695907368152\n",
      "\n",
      "Melhor parametro: {'metric': 'manhattan', 'n_neighbors': 1}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99        67\n",
      "           1       0.94      0.95      0.95        87\n",
      "           2       0.97      0.98      0.98        64\n",
      "           3       0.88      0.89      0.88        80\n",
      "           4       0.96      0.93      0.94        82\n",
      "           5       0.91      0.95      0.93        73\n",
      "           6       0.89      0.93      0.91        73\n",
      "           7       0.96      0.97      0.97        79\n",
      "           8       0.95      0.88      0.91        78\n",
      "           9       0.99      0.99      0.99        82\n",
      "          10       0.99      0.99      0.99        82\n",
      "          11       0.95      0.92      0.93       100\n",
      "          12       0.99      0.94      0.97       104\n",
      "          13       0.92      0.92      0.92        91\n",
      "          14       0.98      0.96      0.97        84\n",
      "          15       1.00      0.96      0.98        79\n",
      "          16       0.93      0.97      0.95        79\n",
      "          17       0.97      0.97      0.97        73\n",
      "          18       0.98      0.96      0.97        90\n",
      "          19       1.00      0.98      0.99        89\n",
      "          20       0.96      0.94      0.95        85\n",
      "          21       0.97      0.97      0.97        72\n",
      "          22       0.95      0.97      0.96        77\n",
      "          23       0.92      1.00      0.96        71\n",
      "          24       0.90      0.94      0.92        84\n",
      "          25       0.97      0.98      0.97        89\n",
      "\n",
      "    accuracy                           0.95      2114\n",
      "   macro avg       0.95      0.96      0.95      2114\n",
      "weighted avg       0.96      0.95      0.95      2114\n",
      "\n",
      "Fold 2: 0.955\n",
      "Media do modelo  0.9547601526262942\n",
      "\n",
      "Melhor parametro: {'metric': 'manhattan', 'n_neighbors': 1}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.98        64\n",
      "           1       0.99      0.94      0.96        97\n",
      "           2       0.99      0.97      0.98        76\n",
      "           3       0.95      0.91      0.93        77\n",
      "           4       0.92      0.96      0.94        89\n",
      "           5       1.00      0.94      0.97        79\n",
      "           6       0.93      0.96      0.95        85\n",
      "           7       1.00      0.97      0.99        74\n",
      "           8       0.90      1.00      0.95        83\n",
      "           9       1.00      0.92      0.96        86\n",
      "          10       0.99      0.99      0.99        87\n",
      "          11       0.88      0.88      0.88        74\n",
      "          12       0.95      0.84      0.89        73\n",
      "          13       0.95      0.95      0.95        77\n",
      "          14       0.95      0.97      0.96        91\n",
      "          15       0.97      0.97      0.97        79\n",
      "          16       0.90      0.96      0.93        74\n",
      "          17       0.96      0.96      0.96        93\n",
      "          18       0.96      0.96      0.96        71\n",
      "          19       0.99      1.00      0.99        81\n",
      "          20       0.99      1.00      0.99        85\n",
      "          21       0.95      0.94      0.94        79\n",
      "          22       0.98      0.99      0.98        90\n",
      "          23       0.96      0.97      0.97       101\n",
      "          24       0.88      0.97      0.92        75\n",
      "          25       0.97      0.95      0.96        74\n",
      "\n",
      "    accuracy                           0.96      2114\n",
      "   macro avg       0.96      0.96      0.96      2114\n",
      "weighted avg       0.96      0.96      0.96      2114\n",
      "\n",
      "Fold 3: 0.956\n",
      "Media do modelo  0.9564142900544362\n",
      "\n",
      "Melhor parametro: {'metric': 'manhattan', 'n_neighbors': 1}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98        94\n",
      "           1       0.96      0.91      0.93        78\n",
      "           2       0.99      0.97      0.98        88\n",
      "           3       0.93      0.94      0.93        82\n",
      "           4       0.94      0.95      0.95        86\n",
      "           5       0.99      0.95      0.97        78\n",
      "           6       0.96      0.95      0.95        74\n",
      "           7       0.99      0.97      0.98        78\n",
      "           8       0.97      0.99      0.98        92\n",
      "           9       0.95      0.97      0.96        78\n",
      "          10       0.99      1.00      0.99        82\n",
      "          11       0.93      0.94      0.94        86\n",
      "          12       0.88      0.93      0.90        68\n",
      "          13       0.95      0.99      0.97        73\n",
      "          14       0.98      0.94      0.96        69\n",
      "          15       0.96      0.99      0.97        77\n",
      "          16       0.98      0.99      0.98        82\n",
      "          17       0.97      0.97      0.97        86\n",
      "          18       0.99      0.98      0.98        86\n",
      "          19       0.99      0.99      0.99        87\n",
      "          20       0.97      1.00      0.99        70\n",
      "          21       0.96      0.92      0.94        74\n",
      "          22       0.97      0.98      0.97        88\n",
      "          23       0.99      0.97      0.98        80\n",
      "          24       0.94      0.90      0.92        87\n",
      "          25       0.98      0.99      0.98        91\n",
      "\n",
      "    accuracy                           0.96      2114\n",
      "   macro avg       0.96      0.96      0.96      2114\n",
      "weighted avg       0.96      0.96      0.96      2114\n",
      "\n",
      "Fold 4: 0.964\n",
      "Media do modelo  0.963665358864068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Melhor parametro: {'metric': 'manhattan', 'n_neighbors': 1}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98        82\n",
      "           1       0.99      0.90      0.94        73\n",
      "           2       0.99      0.99      0.99        76\n",
      "           3       0.90      0.90      0.90        71\n",
      "           4       0.92      0.94      0.93        70\n",
      "           5       0.98      0.96      0.97        85\n",
      "           6       0.93      0.96      0.94        92\n",
      "           7       0.99      1.00      0.99        90\n",
      "           8       0.97      0.96      0.97        72\n",
      "           9       0.96      0.97      0.97        80\n",
      "          10       1.00      1.00      1.00        84\n",
      "          11       0.97      0.88      0.92       104\n",
      "          12       0.95      0.92      0.93        85\n",
      "          13       0.95      0.94      0.94        94\n",
      "          14       0.94      0.93      0.93        67\n",
      "          15       0.96      0.99      0.98        83\n",
      "          16       0.92      1.00      0.96        71\n",
      "          17       1.00      0.97      0.99        71\n",
      "          18       0.96      0.99      0.97        78\n",
      "          19       0.98      1.00      0.99        89\n",
      "          20       0.99      0.98      0.98        90\n",
      "          21       0.96      0.98      0.97        89\n",
      "          22       0.96      0.97      0.97        78\n",
      "          23       0.96      0.94      0.95        86\n",
      "          24       0.92      0.97      0.94        86\n",
      "          25       0.97      0.97      0.97        68\n",
      "\n",
      "    accuracy                           0.96      2114\n",
      "   macro avg       0.96      0.96      0.96      2114\n",
      "weighted avg       0.96      0.96      0.96      2114\n",
      "\n",
      "Fold 5: 0.961\n",
      "Media do modelo  0.9611043462526904\n",
      "\n",
      "Melhor parametro: {'metric': 'manhattan', 'n_neighbors': 1}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99        78\n",
      "           1       0.94      0.93      0.94        72\n",
      "           2       0.98      0.98      0.98        82\n",
      "           3       0.93      0.92      0.92        86\n",
      "           4       0.94      0.94      0.94        83\n",
      "           5       0.99      0.95      0.97        88\n",
      "           6       0.91      0.95      0.93        79\n",
      "           7       0.98      0.97      0.97        86\n",
      "           8       0.93      0.94      0.93        66\n",
      "           9       0.92      0.97      0.94        87\n",
      "          10       0.99      0.99      0.99       103\n",
      "          11       0.93      0.91      0.92        93\n",
      "          12       0.92      1.00      0.96        68\n",
      "          13       0.93      0.93      0.93        73\n",
      "          14       0.97      0.96      0.96        70\n",
      "          15       0.99      0.97      0.98        86\n",
      "          16       0.96      0.95      0.95        99\n",
      "          17       0.96      0.95      0.95        76\n",
      "          18       0.99      0.96      0.97        89\n",
      "          19       0.99      0.97      0.98        80\n",
      "          20       0.95      1.00      0.97        72\n",
      "          21       0.97      0.97      0.97        69\n",
      "          22       1.00      0.95      0.98        85\n",
      "          23       0.93      0.95      0.94        92\n",
      "          24       0.91      0.93      0.92        69\n",
      "          25       0.99      0.98      0.98        83\n",
      "\n",
      "    accuracy                           0.96      2114\n",
      "   macro avg       0.96      0.96      0.96      2114\n",
      "weighted avg       0.96      0.96      0.96      2114\n",
      "\n",
      "Fold 6: 0.957\n",
      "Media do modelo  0.9568603862305465\n",
      "\n",
      "Melhor parametro: {'metric': 'manhattan', 'n_neighbors': 1}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98        90\n",
      "           1       1.00      0.96      0.98        84\n",
      "           2       0.99      0.97      0.98        78\n",
      "           3       0.93      0.89      0.91        94\n",
      "           4       0.99      0.99      0.99        76\n",
      "           5       0.94      0.97      0.96        78\n",
      "           6       0.95      0.99      0.97        82\n",
      "           7       0.97      0.99      0.98        79\n",
      "           8       0.93      0.95      0.94        74\n",
      "           9       0.97      0.99      0.98        68\n",
      "          10       0.98      1.00      0.99        65\n",
      "          11       0.92      0.93      0.93        76\n",
      "          12       0.91      0.94      0.93        88\n",
      "          13       0.93      0.98      0.96        58\n",
      "          14       0.96      0.95      0.96        81\n",
      "          15       1.00      0.96      0.98        83\n",
      "          16       0.97      0.96      0.96        94\n",
      "          17       0.99      0.95      0.97        84\n",
      "          18       0.98      0.95      0.97        88\n",
      "          19       1.00      0.97      0.99        80\n",
      "          20       0.99      0.94      0.96        97\n",
      "          21       0.97      0.97      0.97        87\n",
      "          22       0.97      1.00      0.98        90\n",
      "          23       0.97      0.94      0.96        80\n",
      "          24       0.94      0.96      0.95        92\n",
      "          25       0.90      0.97      0.94        68\n",
      "\n",
      "    accuracy                           0.96      2114\n",
      "   macro avg       0.96      0.96      0.96      2114\n",
      "weighted avg       0.96      0.96      0.96      2114\n",
      "\n",
      "Fold 7: 0.963\n",
      "Media do modelo  0.963097976524871\n",
      "\n",
      "Melhor parametro: {'metric': 'manhattan', 'n_neighbors': 1}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.98      0.97        81\n",
      "           1       0.98      0.97      0.97        96\n",
      "           2       0.98      0.99      0.98        87\n",
      "           3       0.88      0.93      0.90        68\n",
      "           4       0.96      0.97      0.96        93\n",
      "           5       0.96      0.98      0.97        88\n",
      "           6       0.94      0.93      0.94        72\n",
      "           7       0.99      0.98      0.98        89\n",
      "           8       0.92      0.95      0.94        88\n",
      "           9       0.97      0.99      0.98        87\n",
      "          10       0.99      1.00      0.99        70\n",
      "          11       0.97      0.95      0.96        59\n",
      "          12       0.96      0.92      0.94        76\n",
      "          13       0.95      0.97      0.96        71\n",
      "          14       1.00      0.95      0.98       106\n",
      "          15       1.00      1.00      1.00        72\n",
      "          16       0.95      0.99      0.97        75\n",
      "          17       0.95      0.99      0.97        77\n",
      "          18       0.96      0.96      0.96        73\n",
      "          19       0.97      0.99      0.98        74\n",
      "          20       1.00      0.94      0.97        79\n",
      "          21       0.96      0.95      0.96        83\n",
      "          22       0.96      0.94      0.95        82\n",
      "          23       0.95      0.95      0.95        79\n",
      "          24       0.94      0.92      0.93        90\n",
      "          25       0.97      0.95      0.96        98\n",
      "\n",
      "    accuracy                           0.96      2113\n",
      "   macro avg       0.96      0.96      0.96      2113\n",
      "weighted avg       0.96      0.96      0.96      2113\n",
      "\n",
      "Fold 8: 0.962\n",
      "Media do modelo  0.9616544516629338\n",
      "\n",
      "Melhor parametro: {'metric': 'manhattan', 'n_neighbors': 1}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98        97\n",
      "           1       0.94      0.95      0.94        76\n",
      "           2       0.97      0.99      0.98        84\n",
      "           3       0.93      0.91      0.92        88\n",
      "           4       0.95      0.96      0.95        75\n",
      "           5       0.96      0.93      0.94        69\n",
      "           6       0.89      0.90      0.90        93\n",
      "           7       1.00      0.96      0.98        84\n",
      "           8       0.89      0.97      0.93        76\n",
      "           9       0.96      0.94      0.95        72\n",
      "          10       0.99      1.00      0.99        81\n",
      "          11       0.89      0.90      0.90        72\n",
      "          12       0.96      0.98      0.97        88\n",
      "          13       0.98      0.96      0.97        97\n",
      "          14       0.93      0.99      0.96        84\n",
      "          15       0.97      0.95      0.96        77\n",
      "          16       0.98      0.92      0.95        89\n",
      "          17       0.99      0.95      0.97        93\n",
      "          18       0.97      0.99      0.98        76\n",
      "          19       0.97      1.00      0.98        87\n",
      "          20       0.99      0.97      0.98        74\n",
      "          21       1.00      0.97      0.98        91\n",
      "          22       0.98      0.95      0.96        57\n",
      "          23       0.95      0.92      0.93        76\n",
      "          24       0.92      0.97      0.94        69\n",
      "          25       0.98      0.98      0.98        88\n",
      "\n",
      "    accuracy                           0.96      2113\n",
      "   macro avg       0.96      0.96      0.96      2113\n",
      "weighted avg       0.96      0.96      0.96      2113\n",
      "\n",
      "Fold 9: 0.957\n",
      "Media do modelo  0.9572595097024449\n",
      "\n",
      "Precisão média (desvio): 0.958 +- (0.004)\n"
     ]
    }
   ],
   "source": [
    "model = KNeighborsClassifier()\n",
    "parameters = {'n_neighbors': ks,\n",
    "                      'metric':distArray}\n",
    "\n",
    "grid = GridSearchCV(estimator = model,\n",
    "                    param_grid = parameters,\n",
    "                    scoring = 'precision_macro',\n",
    "                    cv = 3)\n",
    "def evaluate_model_with_kfold(kf):\n",
    "    precision_list = []\n",
    "    fold = 0\n",
    "    for train, test in kf.split(X_smt, y_smt):\n",
    "\n",
    "        y_train, y_test = y_smt.iloc[train], y_smt.iloc[test]\n",
    "\n",
    "        X_anova_train = fs.fit_transform(X_smt.iloc[train],y_train)\n",
    "        X_anova_test = fs.transform(X_smt.iloc[test])\n",
    "\n",
    "        grid.fit(X_anova_train, y_train)\n",
    "\n",
    "        y_pred = grid.predict(X_anova_test)\n",
    "\n",
    "        print(\"\\nMelhor parametro:\", grid.best_params_)\n",
    "        print(classification_report(y_test, grid.predict(X_anova_test)))\n",
    "        print(\"Fold %d: %.3f\" %(fold, precision_score(y_test, y_pred,average=\"macro\")))\n",
    "\n",
    "        precision_list.append(precision_score(y_test, y_pred,average=\"macro\"))\n",
    "        print(\"Media do modelo \", precision_list[fold].mean())\n",
    "\n",
    "        fold += 1\n",
    "\n",
    "    precision = np.array(precision_list)\n",
    "    print(\"\\nPrecisão média (desvio): %.3f +- (%.3f)\" %(precision.mean(), precision.std()))\n",
    "\n",
    "# chamada da função de Treino com KFOLD\n",
    "evaluate_model_with_kfold(kFoldObj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TREINAMENTOS MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP - PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Melhor parametro: {'hidden_layer_sizes': 100, 'learning_rate': 'constant', 'learning_rate_init': 0.0001}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.73      0.73        84\n",
      "           1       0.65      0.70      0.68        71\n",
      "           2       0.50      0.34      0.41        93\n",
      "           3       0.62      0.38      0.48        91\n",
      "           4       0.60      0.46      0.52        76\n",
      "           5       0.73      0.70      0.71        86\n",
      "           6       0.43      0.57      0.49        80\n",
      "           7       0.69      0.91      0.79        69\n",
      "           8       0.44      0.57      0.49        92\n",
      "           9       0.73      0.81      0.77        95\n",
      "          10       0.85      0.77      0.81        87\n",
      "          11       0.71      0.51      0.59        77\n",
      "          12       0.45      0.47      0.46        73\n",
      "          13       0.50      0.64      0.56        85\n",
      "          14       0.55      0.62      0.59        96\n",
      "          15       0.75      0.62      0.68        94\n",
      "          16       0.32      0.36      0.34        70\n",
      "          17       0.53      0.38      0.44        89\n",
      "          18       0.81      0.64      0.72        81\n",
      "          19       0.81      0.85      0.83        72\n",
      "          20       0.68      0.68      0.68        76\n",
      "          21       0.56      0.68      0.62        79\n",
      "          22       0.89      0.77      0.83        84\n",
      "          23       0.41      0.51      0.45        63\n",
      "          24       0.44      0.60      0.51        77\n",
      "          25       0.95      0.73      0.82        74\n",
      "\n",
      "    accuracy                           0.61      2114\n",
      "   macro avg       0.63      0.62      0.61      2114\n",
      "weighted avg       0.63      0.61      0.61      2114\n",
      "\n",
      "Fold 0: 0.628\n",
      "Media do modelo  0.6284717359093346\n",
      "\n",
      "Melhor parametro: {'hidden_layer_sizes': 100, 'learning_rate': 'constant', 'learning_rate_init': 0.0001}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.79      0.76        76\n",
      "           1       0.61      0.72      0.66        79\n",
      "           2       0.47      0.33      0.39        85\n",
      "           3       0.55      0.32      0.40        76\n",
      "           4       0.63      0.40      0.49        83\n",
      "           5       0.83      0.66      0.74        89\n",
      "           6       0.30      0.37      0.34        83\n",
      "           7       0.75      0.89      0.82        85\n",
      "           8       0.45      0.60      0.51        92\n",
      "           9       0.66      0.81      0.72        78\n",
      "          10       0.97      0.79      0.87        72\n",
      "          11       0.67      0.42      0.51        72\n",
      "          12       0.64      0.52      0.57        90\n",
      "          13       0.47      0.60      0.53        94\n",
      "          14       0.37      0.62      0.47        65\n",
      "          15       0.65      0.61      0.63        83\n",
      "          16       0.39      0.40      0.40        80\n",
      "          17       0.35      0.31      0.33        71\n",
      "          18       0.79      0.69      0.74        81\n",
      "          19       0.76      0.81      0.78        74\n",
      "          20       0.77      0.68      0.72        85\n",
      "          21       0.62      0.59      0.60        90\n",
      "          22       0.85      0.73      0.78        82\n",
      "          23       0.55      0.53      0.54        85\n",
      "          24       0.40      0.63      0.49        84\n",
      "          25       0.88      0.76      0.82        80\n",
      "\n",
      "    accuracy                           0.60      2114\n",
      "   macro avg       0.62      0.60      0.60      2114\n",
      "weighted avg       0.62      0.60      0.60      2114\n",
      "\n",
      "Fold 1: 0.620\n",
      "Media do modelo  0.6197241032444624\n",
      "\n",
      "Melhor parametro: {'hidden_layer_sizes': 100, 'learning_rate': 'constant', 'learning_rate_init': 0.0001}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.78      0.71        67\n",
      "           1       0.63      0.66      0.64        87\n",
      "           2       0.32      0.36      0.34        64\n",
      "           3       0.55      0.39      0.46        80\n",
      "           4       0.61      0.41      0.49        82\n",
      "           5       0.68      0.74      0.71        73\n",
      "           6       0.28      0.36      0.32        73\n",
      "           7       0.70      0.84      0.76        79\n",
      "           8       0.41      0.59      0.48        78\n",
      "           9       0.68      0.73      0.71        82\n",
      "          10       0.87      0.82      0.84        82\n",
      "          11       0.72      0.26      0.38       100\n",
      "          12       0.59      0.43      0.50       104\n",
      "          13       0.44      0.54      0.49        91\n",
      "          14       0.47      0.57      0.51        84\n",
      "          15       0.68      0.61      0.64        79\n",
      "          16       0.30      0.27      0.28        79\n",
      "          17       0.50      0.42      0.46        73\n",
      "          18       0.74      0.71      0.73        90\n",
      "          19       0.76      0.80      0.78        89\n",
      "          20       0.68      0.62      0.65        85\n",
      "          21       0.52      0.68      0.59        72\n",
      "          22       0.72      0.68      0.70        77\n",
      "          23       0.43      0.51      0.47        71\n",
      "          24       0.50      0.68      0.58        84\n",
      "          25       0.91      0.69      0.78        89\n",
      "\n",
      "    accuracy                           0.58      2114\n",
      "   macro avg       0.59      0.58      0.58      2114\n",
      "weighted avg       0.60      0.58      0.58      2114\n",
      "\n",
      "Fold 2: 0.591\n",
      "Media do modelo  0.5905076494642132\n",
      "\n",
      "Melhor parametro: {'hidden_layer_sizes': 100, 'learning_rate': 'constant', 'learning_rate_init': 0.0001}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.75      0.70        64\n",
      "           1       0.72      0.73      0.73        97\n",
      "           2       0.31      0.22      0.26        76\n",
      "           3       0.65      0.36      0.47        77\n",
      "           4       0.70      0.54      0.61        89\n",
      "           5       0.73      0.68      0.71        79\n",
      "           6       0.35      0.36      0.36        85\n",
      "           7       0.74      0.80      0.77        74\n",
      "           8       0.42      0.64      0.51        83\n",
      "           9       0.64      0.73      0.68        86\n",
      "          10       0.91      0.89      0.90        87\n",
      "          11       0.56      0.32      0.41        74\n",
      "          12       0.58      0.49      0.53        73\n",
      "          13       0.43      0.68      0.53        77\n",
      "          14       0.47      0.57      0.52        91\n",
      "          15       0.69      0.72      0.70        79\n",
      "          16       0.27      0.22      0.24        74\n",
      "          17       0.59      0.38      0.46        93\n",
      "          18       0.83      0.75      0.79        71\n",
      "          19       0.82      0.86      0.84        81\n",
      "          20       0.70      0.71      0.70        85\n",
      "          21       0.48      0.61      0.54        79\n",
      "          22       0.88      0.66      0.75        90\n",
      "          23       0.55      0.51      0.53       101\n",
      "          24       0.38      0.65      0.48        75\n",
      "          25       0.86      0.73      0.79        74\n",
      "\n",
      "    accuracy                           0.60      2114\n",
      "   macro avg       0.61      0.60      0.60      2114\n",
      "weighted avg       0.61      0.60      0.60      2114\n",
      "\n",
      "Fold 3: 0.612\n",
      "Media do modelo  0.6121767299730317\n",
      "\n",
      "Melhor parametro: {'hidden_layer_sizes': 100, 'learning_rate': 'constant', 'learning_rate_init': 0.0001}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.70      0.70        94\n",
      "           1       0.60      0.68      0.64        78\n",
      "           2       0.55      0.34      0.42        88\n",
      "           3       0.60      0.35      0.45        82\n",
      "           4       0.73      0.44      0.55        86\n",
      "           5       0.72      0.72      0.72        78\n",
      "           6       0.32      0.45      0.37        74\n",
      "           7       0.71      0.79      0.75        78\n",
      "           8       0.45      0.61      0.52        92\n",
      "           9       0.66      0.76      0.70        78\n",
      "          10       0.93      0.85      0.89        82\n",
      "          11       0.81      0.45      0.58        86\n",
      "          12       0.49      0.57      0.53        68\n",
      "          13       0.44      0.58      0.50        73\n",
      "          14       0.40      0.61      0.48        69\n",
      "          15       0.66      0.62      0.64        77\n",
      "          16       0.40      0.43      0.41        82\n",
      "          17       0.55      0.42      0.47        86\n",
      "          18       0.83      0.67      0.74        86\n",
      "          19       0.85      0.83      0.84        87\n",
      "          20       0.65      0.70      0.68        70\n",
      "          21       0.49      0.49      0.49        74\n",
      "          22       0.88      0.66      0.75        88\n",
      "          23       0.53      0.53      0.53        80\n",
      "          24       0.41      0.59      0.49        87\n",
      "          25       0.84      0.86      0.85        91\n",
      "\n",
      "    accuracy                           0.60      2114\n",
      "   macro avg       0.62      0.60      0.60      2114\n",
      "weighted avg       0.63      0.60      0.61      2114\n",
      "\n",
      "Fold 4: 0.623\n",
      "Media do modelo  0.6228858061109144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Melhor parametro: {'hidden_layer_sizes': 100, 'learning_rate': 'constant', 'learning_rate_init': 0.0001}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.68      0.68        82\n",
      "           1       0.67      0.63      0.65        73\n",
      "           2       0.49      0.41      0.45        76\n",
      "           3       0.52      0.45      0.48        71\n",
      "           4       0.62      0.47      0.54        70\n",
      "           5       0.69      0.68      0.69        85\n",
      "           6       0.42      0.41      0.42        92\n",
      "           7       0.72      0.87      0.78        90\n",
      "           8       0.38      0.58      0.46        72\n",
      "           9       0.75      0.82      0.79        80\n",
      "          10       0.91      0.76      0.83        84\n",
      "          11       0.81      0.29      0.43       104\n",
      "          12       0.60      0.42      0.50        85\n",
      "          13       0.47      0.56      0.51        94\n",
      "          14       0.38      0.55      0.45        67\n",
      "          15       0.70      0.70      0.70        83\n",
      "          16       0.33      0.39      0.36        71\n",
      "          17       0.45      0.42      0.44        71\n",
      "          18       0.76      0.69      0.72        78\n",
      "          19       0.84      0.89      0.86        89\n",
      "          20       0.70      0.67      0.68        90\n",
      "          21       0.50      0.51      0.50        89\n",
      "          22       0.78      0.69      0.73        78\n",
      "          23       0.51      0.44      0.47        86\n",
      "          24       0.36      0.60      0.45        86\n",
      "          25       0.86      0.74      0.79        68\n",
      "\n",
      "    accuracy                           0.59      2114\n",
      "   macro avg       0.61      0.59      0.59      2114\n",
      "weighted avg       0.62      0.59      0.59      2114\n",
      "\n",
      "Fold 5: 0.611\n",
      "Media do modelo  0.6109507196793645\n",
      "\n",
      "Melhor parametro: {'hidden_layer_sizes': 100, 'learning_rate': 'constant', 'learning_rate_init': 0.0001}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.76      0.73        78\n",
      "           1       0.68      0.60      0.64        72\n",
      "           2       0.45      0.38      0.41        82\n",
      "           3       0.70      0.37      0.48        86\n",
      "           4       0.67      0.46      0.54        83\n",
      "           5       0.73      0.75      0.74        88\n",
      "           6       0.33      0.41      0.36        79\n",
      "           7       0.74      0.81      0.78        86\n",
      "           8       0.35      0.65      0.45        66\n",
      "           9       0.76      0.82      0.78        87\n",
      "          10       0.90      0.82      0.86       103\n",
      "          11       0.85      0.35      0.50        93\n",
      "          12       0.61      0.65      0.63        68\n",
      "          13       0.40      0.63      0.49        73\n",
      "          14       0.35      0.57      0.43        70\n",
      "          15       0.81      0.76      0.78        86\n",
      "          16       0.49      0.33      0.40        99\n",
      "          17       0.61      0.45      0.52        76\n",
      "          18       0.78      0.65      0.71        89\n",
      "          19       0.82      0.85      0.83        80\n",
      "          20       0.67      0.67      0.67        72\n",
      "          21       0.52      0.65      0.58        69\n",
      "          22       0.89      0.75      0.82        85\n",
      "          23       0.60      0.52      0.56        92\n",
      "          24       0.33      0.52      0.40        69\n",
      "          25       0.85      0.82      0.83        83\n",
      "\n",
      "    accuracy                           0.61      2114\n",
      "   macro avg       0.64      0.61      0.61      2114\n",
      "weighted avg       0.65      0.61      0.62      2114\n",
      "\n",
      "Fold 6: 0.637\n",
      "Media do modelo  0.6372303854892787\n",
      "\n",
      "Melhor parametro: {'hidden_layer_sizes': 100, 'learning_rate': 'constant', 'learning_rate_init': 0.0001}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.72      0.72        90\n",
      "           1       0.74      0.65      0.70        84\n",
      "           2       0.45      0.32      0.38        78\n",
      "           3       0.72      0.38      0.50        94\n",
      "           4       0.67      0.54      0.60        76\n",
      "           5       0.68      0.72      0.70        78\n",
      "           6       0.43      0.52      0.47        82\n",
      "           7       0.78      0.85      0.81        79\n",
      "           8       0.38      0.65      0.48        74\n",
      "           9       0.63      0.81      0.71        68\n",
      "          10       0.78      0.75      0.77        65\n",
      "          11       0.67      0.38      0.49        76\n",
      "          12       0.72      0.60      0.65        88\n",
      "          13       0.32      0.64      0.42        58\n",
      "          14       0.51      0.60      0.55        81\n",
      "          15       0.63      0.60      0.62        83\n",
      "          16       0.33      0.33      0.33        94\n",
      "          17       0.49      0.40      0.44        84\n",
      "          18       0.81      0.64      0.71        88\n",
      "          19       0.83      0.89      0.86        80\n",
      "          20       0.77      0.65      0.70        97\n",
      "          21       0.64      0.60      0.62        87\n",
      "          22       0.80      0.73      0.77        90\n",
      "          23       0.47      0.45      0.46        80\n",
      "          24       0.47      0.64      0.54        92\n",
      "          25       0.89      0.82      0.85        68\n",
      "\n",
      "    accuracy                           0.61      2114\n",
      "   macro avg       0.63      0.61      0.61      2114\n",
      "weighted avg       0.63      0.61      0.61      2114\n",
      "\n",
      "Fold 7: 0.628\n",
      "Media do modelo  0.6282626261395142\n",
      "\n",
      "Melhor parametro: {'hidden_layer_sizes': 100, 'learning_rate': 'constant', 'learning_rate_init': 0.0001}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.73      0.75        81\n",
      "           1       0.74      0.72      0.73        96\n",
      "           2       0.47      0.32      0.38        87\n",
      "           3       0.50      0.29      0.37        68\n",
      "           4       0.70      0.57      0.63        93\n",
      "           5       0.80      0.73      0.76        88\n",
      "           6       0.34      0.50      0.40        72\n",
      "           7       0.75      0.85      0.80        89\n",
      "           8       0.45      0.60      0.51        88\n",
      "           9       0.76      0.80      0.78        87\n",
      "          10       0.86      0.80      0.83        70\n",
      "          11       0.68      0.47      0.56        59\n",
      "          12       0.55      0.51      0.53        76\n",
      "          13       0.37      0.59      0.45        71\n",
      "          14       0.45      0.47      0.46       106\n",
      "          15       0.58      0.68      0.63        72\n",
      "          16       0.30      0.31      0.30        75\n",
      "          17       0.53      0.47      0.50        77\n",
      "          18       0.84      0.71      0.77        73\n",
      "          19       0.75      0.84      0.79        74\n",
      "          20       0.68      0.66      0.67        79\n",
      "          21       0.58      0.61      0.60        83\n",
      "          22       0.89      0.66      0.76        82\n",
      "          23       0.54      0.51      0.52        79\n",
      "          24       0.45      0.54      0.49        90\n",
      "          25       0.80      0.68      0.74        98\n",
      "\n",
      "    accuracy                           0.60      2113\n",
      "   macro avg       0.62      0.60      0.60      2113\n",
      "weighted avg       0.62      0.60      0.61      2113\n",
      "\n",
      "Fold 8: 0.620\n",
      "Media do modelo  0.6195867243482442\n",
      "\n",
      "Melhor parametro: {'hidden_layer_sizes': 100, 'learning_rate': 'constant', 'learning_rate_init': 0.0001}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.72      0.69        97\n",
      "           1       0.58      0.66      0.62        76\n",
      "           2       0.42      0.33      0.37        84\n",
      "           3       0.62      0.34      0.44        88\n",
      "           4       0.53      0.53      0.53        75\n",
      "           5       0.60      0.71      0.65        69\n",
      "           6       0.35      0.33      0.34        93\n",
      "           7       0.83      0.86      0.84        84\n",
      "           8       0.46      0.62      0.53        76\n",
      "           9       0.72      0.79      0.75        72\n",
      "          10       0.88      0.85      0.87        81\n",
      "          11       0.59      0.40      0.48        72\n",
      "          12       0.67      0.43      0.52        88\n",
      "          13       0.47      0.64      0.54        97\n",
      "          14       0.39      0.64      0.49        84\n",
      "          15       0.64      0.64      0.64        77\n",
      "          16       0.32      0.24      0.27        89\n",
      "          17       0.62      0.41      0.49        93\n",
      "          18       0.70      0.61      0.65        76\n",
      "          19       0.78      0.87      0.83        87\n",
      "          20       0.72      0.70      0.71        74\n",
      "          21       0.53      0.54      0.53        91\n",
      "          22       0.76      0.61      0.68        57\n",
      "          23       0.55      0.54      0.55        76\n",
      "          24       0.37      0.61      0.46        69\n",
      "          25       0.90      0.75      0.82        88\n",
      "\n",
      "    accuracy                           0.59      2113\n",
      "   macro avg       0.60      0.59      0.59      2113\n",
      "weighted avg       0.60      0.59      0.58      2113\n",
      "\n",
      "Fold 9: 0.603\n",
      "Media do modelo  0.6032845893403915\n"
     ]
    }
   ],
   "source": [
    "model = MLPClassifier()\n",
    "train_score = []\n",
    "val_score = []\n",
    "\n",
    "parameters = {'hidden_layer_sizes':n,\n",
    "              'learning_rate': learningRate,\n",
    "              'learning_rate_init': learningRateInit}\n",
    "\n",
    "grid = GridSearchCV(estimator = model,\n",
    "                    param_grid = parameters,\n",
    "                    scoring = 'precision_macro',\n",
    "                    cv = 2)\n",
    "\n",
    "def evaluate_model_with_kfold(kf):\n",
    "    precision_list = []\n",
    "    fold = 0\n",
    "    for train, test in kf.split(X_smt, y_smt):\n",
    "\n",
    "\n",
    "        y_train, y_test = y_smt.iloc[train], y_smt.iloc[test]\n",
    "\n",
    "        X_pca_train = pca.fit_transform(X_smt.iloc[train])\n",
    "        X_pca_test = pca.transform(X_smt.iloc[test])\n",
    "\n",
    "        grid.fit(X_pca_train, y_train)\n",
    "\n",
    "        y_pred = grid.predict(X_pca_test)\n",
    "\n",
    "        print(\"\\nMelhor parametro:\", grid.best_params_)\n",
    "        print(classification_report(y_test, grid.predict(X_pca_test)))\n",
    "        print(\"Fold %d: %.3f\" %(fold, precision_score(y_test, y_pred,average=\"macro\")))\n",
    "\n",
    "        precision_list.append(precision_score(y_test, y_pred,average=\"macro\"))\n",
    "        print(\"Media do modelo \", precision_list[fold].mean())\n",
    "\n",
    "        fold += 1\n",
    "\n",
    "# chamada da função de Treino com KFOLD\n",
    "evaluate_model_with_kfold(kFoldObj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP - LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Melhor parametro: {'hidden_layer_sizes': 100, 'learning_rate': 'constant', 'learning_rate_init': 0.0001}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.94      0.96        84\n",
      "           1       0.96      0.96      0.96        71\n",
      "           2       0.92      0.86      0.89        93\n",
      "           3       0.83      0.82      0.83        91\n",
      "           4       0.91      0.88      0.89        76\n",
      "           5       0.92      0.92      0.92        86\n",
      "           6       0.82      0.84      0.83        80\n",
      "           7       0.93      0.96      0.94        69\n",
      "           8       0.92      0.89      0.91        92\n",
      "           9       0.96      0.95      0.95        95\n",
      "          10       0.94      0.98      0.96        87\n",
      "          11       0.92      0.90      0.91        77\n",
      "          12       0.88      0.90      0.89        73\n",
      "          13       0.88      0.89      0.89        85\n",
      "          14       0.92      0.98      0.95        96\n",
      "          15       0.97      0.96      0.96        94\n",
      "          16       0.93      0.91      0.92        70\n",
      "          17       0.95      0.97      0.96        89\n",
      "          18       0.95      0.89      0.92        81\n",
      "          19       0.97      0.96      0.97        72\n",
      "          20       0.97      0.95      0.96        76\n",
      "          21       0.92      0.97      0.94        79\n",
      "          22       0.95      0.93      0.94        84\n",
      "          23       0.89      0.92      0.91        63\n",
      "          24       0.85      0.94      0.89        77\n",
      "          25       0.96      0.92      0.94        74\n",
      "\n",
      "    accuracy                           0.92      2114\n",
      "   macro avg       0.92      0.92      0.92      2114\n",
      "weighted avg       0.92      0.92      0.92      2114\n",
      "\n",
      "Fold 0: 0.922\n",
      "Media do modelo  0.922408583359117\n",
      "\n",
      "Melhor parametro: {'hidden_layer_sizes': 100, 'learning_rate': 'constant', 'learning_rate_init': 0.0001}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.93      0.94        76\n",
      "           1       0.96      0.92      0.94        79\n",
      "           2       0.95      0.96      0.96        85\n",
      "           3       0.83      0.72      0.77        76\n",
      "           4       0.90      0.83      0.86        83\n",
      "           5       0.96      0.92      0.94        89\n",
      "           6       0.81      0.90      0.85        83\n",
      "           7       0.90      0.94      0.92        85\n",
      "           8       0.88      0.92      0.90        92\n",
      "           9       0.97      0.92      0.95        78\n",
      "          10       0.99      0.93      0.96        72\n",
      "          11       0.82      0.89      0.85        72\n",
      "          12       0.90      0.83      0.87        90\n",
      "          13       0.94      0.90      0.92        94\n",
      "          14       0.88      0.94      0.91        65\n",
      "          15       0.96      0.87      0.91        83\n",
      "          16       0.88      0.95      0.92        80\n",
      "          17       0.91      0.96      0.93        71\n",
      "          18       0.97      0.94      0.96        81\n",
      "          19       0.96      0.93      0.95        74\n",
      "          20       0.92      0.98      0.95        85\n",
      "          21       0.92      0.93      0.93        90\n",
      "          22       0.93      0.94      0.93        82\n",
      "          23       0.86      0.84      0.85        85\n",
      "          24       0.83      0.94      0.88        84\n",
      "          25       0.96      0.96      0.96        80\n",
      "\n",
      "    accuracy                           0.91      2114\n",
      "   macro avg       0.91      0.91      0.91      2114\n",
      "weighted avg       0.91      0.91      0.91      2114\n",
      "\n",
      "Fold 1: 0.914\n",
      "Media do modelo  0.9135741713589789\n",
      "\n",
      "Melhor parametro: {'hidden_layer_sizes': 100, 'learning_rate': 'constant', 'learning_rate_init': 0.0001}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98        67\n",
      "           1       0.96      0.90      0.93        87\n",
      "           2       0.82      0.92      0.87        64\n",
      "           3       0.90      0.82      0.86        80\n",
      "           4       0.86      0.87      0.86        82\n",
      "           5       0.87      0.93      0.90        73\n",
      "           6       0.81      0.81      0.81        73\n",
      "           7       0.93      0.94      0.93        79\n",
      "           8       0.79      0.85      0.81        78\n",
      "           9       0.96      0.91      0.94        82\n",
      "          10       0.97      0.95      0.96        82\n",
      "          11       0.93      0.90      0.91       100\n",
      "          12       0.95      0.89      0.92       104\n",
      "          13       0.87      0.88      0.87        91\n",
      "          14       0.93      0.92      0.92        84\n",
      "          15       0.97      0.92      0.95        79\n",
      "          16       0.90      0.95      0.93        79\n",
      "          17       0.93      0.93      0.93        73\n",
      "          18       0.95      0.91      0.93        90\n",
      "          19       0.90      0.96      0.93        89\n",
      "          20       0.95      0.95      0.95        85\n",
      "          21       0.94      0.92      0.93        72\n",
      "          22       0.95      0.95      0.95        77\n",
      "          23       0.80      0.89      0.84        71\n",
      "          24       0.88      0.89      0.89        84\n",
      "          25       0.94      0.90      0.92        89\n",
      "\n",
      "    accuracy                           0.91      2114\n",
      "   macro avg       0.91      0.91      0.91      2114\n",
      "weighted avg       0.91      0.91      0.91      2114\n",
      "\n",
      "Fold 2: 0.909\n",
      "Media do modelo  0.9089032848812514\n",
      "\n",
      "Melhor parametro: {'hidden_layer_sizes': 100, 'learning_rate': 'constant', 'learning_rate_init': 0.0001}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.92      0.94        64\n",
      "           1       0.96      0.97      0.96        97\n",
      "           2       0.91      0.88      0.89        76\n",
      "           3       0.81      0.83      0.82        77\n",
      "           4       0.91      0.93      0.92        89\n",
      "           5       0.96      0.94      0.95        79\n",
      "           6       0.79      0.85      0.82        85\n",
      "           7       0.93      0.92      0.93        74\n",
      "           8       0.87      0.94      0.90        83\n",
      "           9       0.93      0.88      0.90        86\n",
      "          10       0.98      0.99      0.98        87\n",
      "          11       0.88      0.80      0.84        74\n",
      "          12       0.86      0.89      0.87        73\n",
      "          13       0.87      0.96      0.91        77\n",
      "          14       0.92      0.92      0.92        91\n",
      "          15       0.95      0.94      0.94        79\n",
      "          16       0.91      0.93      0.92        74\n",
      "          17       0.98      0.90      0.94        93\n",
      "          18       0.96      0.92      0.94        71\n",
      "          19       0.94      0.90      0.92        81\n",
      "          20       0.93      0.94      0.94        85\n",
      "          21       0.96      0.95      0.96        79\n",
      "          22       0.95      0.93      0.94        90\n",
      "          23       0.87      0.89      0.88       101\n",
      "          24       0.88      0.89      0.89        75\n",
      "          25       0.93      0.92      0.93        74\n",
      "\n",
      "    accuracy                           0.91      2114\n",
      "   macro avg       0.91      0.91      0.91      2114\n",
      "weighted avg       0.92      0.91      0.91      2114\n",
      "\n",
      "Fold 3: 0.915\n",
      "Media do modelo  0.9146457850220059\n",
      "\n",
      "Melhor parametro: {'hidden_layer_sizes': 100, 'learning_rate': 'constant', 'learning_rate_init': 0.0001}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.88      0.89        94\n",
      "           1       0.95      0.81      0.88        78\n",
      "           2       0.89      0.90      0.89        88\n",
      "           3       0.90      0.84      0.87        82\n",
      "           4       0.87      0.90      0.88        86\n",
      "           5       0.96      0.92      0.94        78\n",
      "           6       0.83      0.91      0.86        74\n",
      "           7       0.91      0.94      0.92        78\n",
      "           8       0.89      0.89      0.89        92\n",
      "           9       0.94      0.95      0.94        78\n",
      "          10       0.96      0.91      0.94        82\n",
      "          11       0.89      0.90      0.89        86\n",
      "          12       0.87      0.91      0.89        68\n",
      "          13       0.88      0.90      0.89        73\n",
      "          14       0.88      0.87      0.88        69\n",
      "          15       0.96      0.97      0.97        77\n",
      "          16       0.90      0.98      0.94        82\n",
      "          17       0.93      0.93      0.93        86\n",
      "          18       0.98      0.97      0.97        86\n",
      "          19       0.95      0.93      0.94        87\n",
      "          20       0.92      0.96      0.94        70\n",
      "          21       0.96      0.88      0.92        74\n",
      "          22       0.93      0.91      0.92        88\n",
      "          23       0.88      0.89      0.88        80\n",
      "          24       0.85      0.92      0.88        87\n",
      "          25       0.97      0.95      0.96        91\n",
      "\n",
      "    accuracy                           0.91      2114\n",
      "   macro avg       0.91      0.91      0.91      2114\n",
      "weighted avg       0.91      0.91      0.91      2114\n",
      "\n",
      "Fold 4: 0.912\n",
      "Media do modelo  0.9124490370898852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Melhor parametro: {'hidden_layer_sizes': 100, 'learning_rate': 'constant', 'learning_rate_init': 0.0001}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.89      0.92        82\n",
      "           1       0.94      0.84      0.88        73\n",
      "           2       0.87      0.97      0.92        76\n",
      "           3       0.85      0.89      0.87        71\n",
      "           4       0.86      0.91      0.89        70\n",
      "           5       0.91      0.93      0.92        85\n",
      "           6       0.82      0.89      0.85        92\n",
      "           7       1.00      0.96      0.98        90\n",
      "           8       0.89      0.90      0.90        72\n",
      "           9       0.94      0.95      0.94        80\n",
      "          10       0.95      0.95      0.95        84\n",
      "          11       0.93      0.92      0.93       104\n",
      "          12       0.92      0.91      0.91        85\n",
      "          13       0.94      0.88      0.91        94\n",
      "          14       0.89      0.87      0.88        67\n",
      "          15       0.99      0.94      0.96        83\n",
      "          16       0.93      0.96      0.94        71\n",
      "          17       0.96      0.93      0.94        71\n",
      "          18       0.92      0.92      0.92        78\n",
      "          19       0.94      1.00      0.97        89\n",
      "          20       0.97      0.94      0.96        90\n",
      "          21       0.95      0.90      0.92        89\n",
      "          22       0.95      0.91      0.93        78\n",
      "          23       0.85      0.91      0.88        86\n",
      "          24       0.92      0.93      0.92        86\n",
      "          25       0.96      0.96      0.96        68\n",
      "\n",
      "    accuracy                           0.92      2114\n",
      "   macro avg       0.92      0.92      0.92      2114\n",
      "weighted avg       0.92      0.92      0.92      2114\n",
      "\n",
      "Fold 5: 0.923\n",
      "Media do modelo  0.9226968158478166\n",
      "\n",
      "Melhor parametro: {'hidden_layer_sizes': 100, 'learning_rate': 'constant', 'learning_rate_init': 0.0001}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.97        78\n",
      "           1       0.97      0.89      0.93        72\n",
      "           2       0.90      0.85      0.88        82\n",
      "           3       0.94      0.85      0.89        86\n",
      "           4       0.91      0.90      0.91        83\n",
      "           5       0.93      0.88      0.90        88\n",
      "           6       0.80      0.94      0.87        79\n",
      "           7       0.96      0.88      0.92        86\n",
      "           8       0.80      0.95      0.87        66\n",
      "           9       0.96      0.93      0.95        87\n",
      "          10       0.95      0.96      0.96       103\n",
      "          11       0.88      0.86      0.87        93\n",
      "          12       0.88      0.99      0.93        68\n",
      "          13       0.86      0.96      0.91        73\n",
      "          14       0.97      0.91      0.94        70\n",
      "          15       0.94      0.94      0.94        86\n",
      "          16       0.94      0.93      0.93        99\n",
      "          17       0.95      0.97      0.96        76\n",
      "          18       0.96      0.90      0.93        89\n",
      "          19       0.94      0.96      0.95        80\n",
      "          20       0.96      0.96      0.96        72\n",
      "          21       0.94      0.94      0.94        69\n",
      "          22       0.97      0.91      0.94        85\n",
      "          23       0.87      0.90      0.89        92\n",
      "          24       0.81      0.84      0.82        69\n",
      "          25       0.96      0.94      0.95        83\n",
      "\n",
      "    accuracy                           0.92      2114\n",
      "   macro avg       0.92      0.92      0.92      2114\n",
      "weighted avg       0.92      0.92      0.92      2114\n",
      "\n",
      "Fold 6: 0.920\n",
      "Media do modelo  0.9204616244667586\n",
      "\n",
      "Melhor parametro: {'hidden_layer_sizes': 100, 'learning_rate': 'constant', 'learning_rate_init': 0.0001}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.90      0.93        90\n",
      "           1       0.96      0.88      0.92        84\n",
      "           2       0.79      0.91      0.85        78\n",
      "           3       0.93      0.80      0.86        94\n",
      "           4       0.88      0.91      0.90        76\n",
      "           5       0.96      0.94      0.95        78\n",
      "           6       0.89      0.91      0.90        82\n",
      "           7       0.90      0.94      0.92        79\n",
      "           8       0.82      0.85      0.83        74\n",
      "           9       0.92      0.90      0.91        68\n",
      "          10       0.97      0.94      0.95        65\n",
      "          11       0.92      0.95      0.94        76\n",
      "          12       0.86      0.88      0.87        88\n",
      "          13       0.88      0.90      0.89        58\n",
      "          14       0.92      0.96      0.94        81\n",
      "          15       0.99      0.93      0.96        83\n",
      "          16       0.95      0.86      0.91        94\n",
      "          17       0.86      0.92      0.89        84\n",
      "          18       0.93      0.91      0.92        88\n",
      "          19       0.95      0.93      0.94        80\n",
      "          20       0.94      0.97      0.95        97\n",
      "          21       0.99      0.92      0.95        87\n",
      "          22       0.89      0.93      0.91        90\n",
      "          23       0.88      0.81      0.84        80\n",
      "          24       0.84      0.97      0.90        92\n",
      "          25       0.92      0.96      0.94        68\n",
      "\n",
      "    accuracy                           0.91      2114\n",
      "   macro avg       0.91      0.91      0.91      2114\n",
      "weighted avg       0.91      0.91      0.91      2114\n",
      "\n",
      "Fold 7: 0.911\n",
      "Media do modelo  0.9110360979336873\n",
      "\n",
      "Melhor parametro: {'hidden_layer_sizes': 100, 'learning_rate': 'constant', 'learning_rate_init': 0.0001}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.96      0.95        81\n",
      "           1       0.89      0.92      0.90        96\n",
      "           2       0.93      0.91      0.92        87\n",
      "           3       0.95      0.87      0.91        68\n",
      "           4       0.89      0.86      0.87        93\n",
      "           5       0.95      0.95      0.95        88\n",
      "           6       0.89      0.92      0.90        72\n",
      "           7       0.94      0.94      0.94        89\n",
      "           8       0.92      0.92      0.92        88\n",
      "           9       0.95      0.92      0.94        87\n",
      "          10       0.99      0.96      0.97        70\n",
      "          11       0.82      0.92      0.86        59\n",
      "          12       0.91      0.88      0.89        76\n",
      "          13       0.93      0.96      0.94        71\n",
      "          14       0.93      0.98      0.95       106\n",
      "          15       0.99      0.93      0.96        72\n",
      "          16       0.86      0.93      0.90        75\n",
      "          17       0.91      0.88      0.89        77\n",
      "          18       0.91      0.92      0.91        73\n",
      "          19       0.93      0.93      0.93        74\n",
      "          20       0.97      0.96      0.97        79\n",
      "          21       0.95      0.98      0.96        83\n",
      "          22       0.95      0.94      0.94        82\n",
      "          23       0.89      0.90      0.89        79\n",
      "          24       0.89      0.89      0.89        90\n",
      "          25       0.97      0.91      0.94        98\n",
      "\n",
      "    accuracy                           0.92      2113\n",
      "   macro avg       0.92      0.92      0.92      2113\n",
      "weighted avg       0.93      0.92      0.92      2113\n",
      "\n",
      "Fold 8: 0.925\n",
      "Media do modelo  0.9248579981153623\n",
      "\n",
      "Melhor parametro: {'hidden_layer_sizes': 100, 'learning_rate': 'constant', 'learning_rate_init': 0.0001}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.95      0.94        97\n",
      "           1       0.96      0.91      0.93        76\n",
      "           2       0.88      0.90      0.89        84\n",
      "           3       0.86      0.81      0.83        88\n",
      "           4       0.85      0.93      0.89        75\n",
      "           5       0.95      0.86      0.90        69\n",
      "           6       0.79      0.87      0.83        93\n",
      "           7       0.92      0.98      0.95        84\n",
      "           8       0.87      0.93      0.90        76\n",
      "           9       0.92      0.94      0.93        72\n",
      "          10       0.98      0.99      0.98        81\n",
      "          11       0.89      0.90      0.90        72\n",
      "          12       0.92      0.90      0.91        88\n",
      "          13       0.95      0.95      0.95        97\n",
      "          14       0.94      0.95      0.95        84\n",
      "          15       0.95      0.90      0.92        77\n",
      "          16       0.95      0.90      0.92        89\n",
      "          17       0.98      0.91      0.94        93\n",
      "          18       0.92      0.87      0.89        76\n",
      "          19       0.93      0.97      0.95        87\n",
      "          20       0.97      0.93      0.95        74\n",
      "          21       0.94      0.90      0.92        91\n",
      "          22       0.91      0.88      0.89        57\n",
      "          23       0.82      0.86      0.84        76\n",
      "          24       0.80      0.87      0.83        69\n",
      "          25       0.97      0.95      0.96        88\n",
      "\n",
      "    accuracy                           0.91      2113\n",
      "   macro avg       0.91      0.91      0.91      2113\n",
      "weighted avg       0.91      0.91      0.91      2113\n",
      "\n",
      "Fold 9: 0.914\n",
      "Media do modelo  0.9135525958722953\n",
      "\n",
      "Precisão média (desvio): 0.916 +- (0.005)\n"
     ]
    }
   ],
   "source": [
    "model = MLPClassifier()\n",
    "train_score = []\n",
    "val_score = []\n",
    "\n",
    "parameters = {'hidden_layer_sizes':n,\n",
    "              'learning_rate': learningRate,\n",
    "              'learning_rate_init': learningRateInit}\n",
    "\n",
    "grid = GridSearchCV(estimator = model,\n",
    "                    param_grid = parameters,\n",
    "                    scoring = 'precision_macro',\n",
    "                    cv = 2)\n",
    "\n",
    "def evaluate_model_with_kfold(kf):\n",
    "    precision_list = []\n",
    "    fold = 0\n",
    "    for train, test in kf.split(X_smt, y_smt):\n",
    "\n",
    "        y_train, y_test = y_smt.iloc[train], y_smt.iloc[test]\n",
    "\n",
    "        X_lda_train = lda.fit_transform(X_smt.iloc[train],y_train)\n",
    "        X_lda_test = lda.transform(X_smt.iloc[test])\n",
    "\n",
    "        grid.fit(X_lda_train, y_train)\n",
    "\n",
    "        y_pred = grid.predict(X_lda_test)\n",
    "\n",
    "        print(\"\\nMelhor parametro:\", grid.best_params_)\n",
    "        print(classification_report(y_test, grid.predict(X_lda_test)))\n",
    "        print(\"Fold %d: %.3f\" %(fold, precision_score(y_test, y_pred,average=\"macro\")))\n",
    "\n",
    "        precision_list.append(precision_score(y_test, y_pred,average=\"macro\"))\n",
    "        print(\"Media do modelo \", precision_list[fold].mean())\n",
    "\n",
    "        fold += 1\n",
    "\n",
    "    precision = np.array(precision_list)\n",
    "    print(\"\\nPrecisão média (desvio): %.3f +- (%.3f)\" %(precision.mean(), precision.std()))\n",
    "\n",
    "# chamada da função de Treino com KFOLD\n",
    "evaluate_model_with_kfold(kFoldObj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP - ANOVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Melhor parametro: {'hidden_layer_sizes': 100, 'learning_rate': 'constant', 'learning_rate_init': 0.0001}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.95      0.93        84\n",
      "           1       0.96      0.90      0.93        71\n",
      "           2       0.89      0.81      0.85        93\n",
      "           3       0.86      0.73      0.79        91\n",
      "           4       0.86      0.84      0.85        76\n",
      "           5       0.93      0.90      0.91        86\n",
      "           6       0.81      0.82      0.82        80\n",
      "           7       0.88      0.93      0.90        69\n",
      "           8       0.90      0.87      0.88        92\n",
      "           9       0.95      0.95      0.95        95\n",
      "          10       0.91      0.95      0.93        87\n",
      "          11       0.83      0.88      0.86        77\n",
      "          12       0.82      0.79      0.81        73\n",
      "          13       0.85      0.88      0.87        85\n",
      "          14       0.92      0.95      0.93        96\n",
      "          15       0.96      0.94      0.95        94\n",
      "          16       0.93      0.90      0.91        70\n",
      "          17       0.94      0.93      0.94        89\n",
      "          18       0.96      0.88      0.92        81\n",
      "          19       0.91      0.93      0.92        72\n",
      "          20       0.99      0.93      0.96        76\n",
      "          21       0.88      0.95      0.91        79\n",
      "          22       0.92      0.90      0.91        84\n",
      "          23       0.76      0.89      0.82        63\n",
      "          24       0.79      0.94      0.86        77\n",
      "          25       0.94      0.91      0.92        74\n",
      "\n",
      "    accuracy                           0.89      2114\n",
      "   macro avg       0.89      0.89      0.89      2114\n",
      "weighted avg       0.90      0.89      0.89      2114\n",
      "\n",
      "Fold 0: 0.894\n",
      "Media do modelo  0.89389145999386\n",
      "\n",
      "Melhor parametro: {'hidden_layer_sizes': 100, 'learning_rate': 'constant', 'learning_rate_init': 0.0001}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.87      0.90        76\n",
      "           1       0.95      0.89      0.92        79\n",
      "           2       0.85      0.88      0.87        85\n",
      "           3       0.80      0.70      0.75        76\n",
      "           4       0.86      0.81      0.83        83\n",
      "           5       0.95      0.91      0.93        89\n",
      "           6       0.84      0.90      0.87        83\n",
      "           7       0.89      0.94      0.91        85\n",
      "           8       0.89      0.90      0.90        92\n",
      "           9       0.95      0.91      0.93        78\n",
      "          10       0.94      0.92      0.93        72\n",
      "          11       0.84      0.86      0.85        72\n",
      "          12       0.91      0.86      0.88        90\n",
      "          13       0.91      0.84      0.87        94\n",
      "          14       0.81      0.89      0.85        65\n",
      "          15       0.90      0.88      0.89        83\n",
      "          16       0.83      0.94      0.88        80\n",
      "          17       0.89      0.94      0.92        71\n",
      "          18       0.96      0.86      0.91        81\n",
      "          19       0.94      0.91      0.92        74\n",
      "          20       0.89      0.95      0.92        85\n",
      "          21       0.88      0.93      0.91        90\n",
      "          22       0.94      0.93      0.93        82\n",
      "          23       0.79      0.81      0.80        85\n",
      "          24       0.82      0.93      0.87        84\n",
      "          25       0.96      0.94      0.95        80\n",
      "\n",
      "    accuracy                           0.89      2114\n",
      "   macro avg       0.89      0.89      0.89      2114\n",
      "weighted avg       0.89      0.89      0.89      2114\n",
      "\n",
      "Fold 1: 0.890\n",
      "Media do modelo  0.8897076932492161\n",
      "\n",
      "Melhor parametro: {'hidden_layer_sizes': 100, 'learning_rate': 'constant', 'learning_rate_init': 0.0001}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.97      0.96        67\n",
      "           1       0.93      0.86      0.89        87\n",
      "           2       0.78      0.89      0.83        64\n",
      "           3       0.89      0.79      0.83        80\n",
      "           4       0.88      0.88      0.88        82\n",
      "           5       0.89      0.90      0.90        73\n",
      "           6       0.85      0.90      0.87        73\n",
      "           7       0.94      0.96      0.95        79\n",
      "           8       0.80      0.85      0.82        78\n",
      "           9       0.91      0.91      0.91        82\n",
      "          10       0.95      0.96      0.96        82\n",
      "          11       0.92      0.85      0.89       100\n",
      "          12       0.92      0.89      0.91       104\n",
      "          13       0.86      0.89      0.88        91\n",
      "          14       0.91      0.94      0.92        84\n",
      "          15       0.99      0.90      0.94        79\n",
      "          16       0.93      0.95      0.94        79\n",
      "          17       0.93      0.95      0.94        73\n",
      "          18       0.93      0.89      0.91        90\n",
      "          19       0.91      0.96      0.93        89\n",
      "          20       0.96      0.96      0.96        85\n",
      "          21       0.92      0.93      0.92        72\n",
      "          22       0.92      0.92      0.92        77\n",
      "          23       0.78      0.85      0.81        71\n",
      "          24       0.90      0.90      0.90        84\n",
      "          25       0.94      0.87      0.90        89\n",
      "\n",
      "    accuracy                           0.90      2114\n",
      "   macro avg       0.90      0.90      0.90      2114\n",
      "weighted avg       0.91      0.90      0.90      2114\n",
      "\n",
      "Fold 2: 0.904\n",
      "Media do modelo  0.9037182772839708\n",
      "\n",
      "Melhor parametro: {'hidden_layer_sizes': 100, 'learning_rate': 'constant', 'learning_rate_init': 0.0001}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.95      0.95        64\n",
      "           1       0.91      0.96      0.93        97\n",
      "           2       0.86      0.84      0.85        76\n",
      "           3       0.78      0.81      0.79        77\n",
      "           4       0.92      0.85      0.88        89\n",
      "           5       0.93      0.90      0.92        79\n",
      "           6       0.82      0.85      0.83        85\n",
      "           7       0.93      0.92      0.93        74\n",
      "           8       0.89      0.89      0.89        83\n",
      "           9       0.91      0.84      0.87        86\n",
      "          10       0.95      0.97      0.96        87\n",
      "          11       0.90      0.74      0.81        74\n",
      "          12       0.81      0.85      0.83        73\n",
      "          13       0.79      0.95      0.86        77\n",
      "          14       0.94      0.93      0.94        91\n",
      "          15       0.95      0.94      0.94        79\n",
      "          16       0.90      0.95      0.92        74\n",
      "          17       0.96      0.91      0.93        93\n",
      "          18       0.91      0.87      0.89        71\n",
      "          19       0.95      0.91      0.93        81\n",
      "          20       0.89      0.92      0.90        85\n",
      "          21       0.97      0.95      0.96        79\n",
      "          22       0.88      0.91      0.90        90\n",
      "          23       0.89      0.82      0.86       101\n",
      "          24       0.76      0.88      0.81        75\n",
      "          25       0.95      0.96      0.95        74\n",
      "\n",
      "    accuracy                           0.89      2114\n",
      "   macro avg       0.90      0.89      0.89      2114\n",
      "weighted avg       0.90      0.89      0.90      2114\n",
      "\n",
      "Fold 3: 0.896\n",
      "Media do modelo  0.8962802790490072\n",
      "\n",
      "Melhor parametro: {'hidden_layer_sizes': 100, 'learning_rate': 'constant', 'learning_rate_init': 0.0001}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.94      0.93        94\n",
      "           1       0.91      0.79      0.85        78\n",
      "           2       0.89      0.93      0.91        88\n",
      "           3       0.87      0.79      0.83        82\n",
      "           4       0.87      0.83      0.85        86\n",
      "           5       0.92      0.91      0.92        78\n",
      "           6       0.80      0.89      0.85        74\n",
      "           7       0.89      0.91      0.90        78\n",
      "           8       0.86      0.90      0.88        92\n",
      "           9       0.96      0.92      0.94        78\n",
      "          10       0.96      0.93      0.94        82\n",
      "          11       0.85      0.87      0.86        86\n",
      "          12       0.84      0.87      0.86        68\n",
      "          13       0.84      0.89      0.87        73\n",
      "          14       0.88      0.87      0.88        69\n",
      "          15       0.94      0.95      0.94        77\n",
      "          16       0.90      0.94      0.92        82\n",
      "          17       0.93      0.93      0.93        86\n",
      "          18       0.96      0.95      0.96        86\n",
      "          19       0.94      0.93      0.94        87\n",
      "          20       0.90      0.94      0.92        70\n",
      "          21       0.91      0.86      0.89        74\n",
      "          22       0.92      0.91      0.91        88\n",
      "          23       0.84      0.82      0.83        80\n",
      "          24       0.85      0.92      0.88        87\n",
      "          25       0.99      0.92      0.95        91\n",
      "\n",
      "    accuracy                           0.90      2114\n",
      "   macro avg       0.90      0.90      0.90      2114\n",
      "weighted avg       0.90      0.90      0.90      2114\n",
      "\n",
      "Fold 4: 0.898\n",
      "Media do modelo  0.8983237726177452\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Melhor parametro: {'hidden_layer_sizes': 100, 'learning_rate': 'constant', 'learning_rate_init': 0.0001}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.88      0.90        82\n",
      "           1       0.94      0.81      0.87        73\n",
      "           2       0.78      0.89      0.83        76\n",
      "           3       0.87      0.85      0.86        71\n",
      "           4       0.81      0.90      0.85        70\n",
      "           5       0.87      0.89      0.88        85\n",
      "           6       0.81      0.90      0.86        92\n",
      "           7       0.96      0.98      0.97        90\n",
      "           8       0.85      0.88      0.86        72\n",
      "           9       0.95      0.91      0.93        80\n",
      "          10       0.93      0.96      0.95        84\n",
      "          11       0.93      0.88      0.90       104\n",
      "          12       0.80      0.85      0.82        85\n",
      "          13       0.93      0.86      0.90        94\n",
      "          14       0.88      0.91      0.90        67\n",
      "          15       0.99      0.89      0.94        83\n",
      "          16       0.88      0.90      0.89        71\n",
      "          17       0.93      0.92      0.92        71\n",
      "          18       0.95      0.94      0.94        78\n",
      "          19       0.96      0.98      0.97        89\n",
      "          20       0.93      0.94      0.94        90\n",
      "          21       0.96      0.85      0.90        89\n",
      "          22       0.90      0.91      0.90        78\n",
      "          23       0.85      0.91      0.88        86\n",
      "          24       0.87      0.85      0.86        86\n",
      "          25       0.92      0.88      0.90        68\n",
      "\n",
      "    accuracy                           0.90      2114\n",
      "   macro avg       0.90      0.90      0.90      2114\n",
      "weighted avg       0.90      0.90      0.90      2114\n",
      "\n",
      "Fold 5: 0.899\n",
      "Media do modelo  0.8987379260466752\n",
      "\n",
      "Melhor parametro: {'hidden_layer_sizes': 100, 'learning_rate': 'constant', 'learning_rate_init': 0.0001}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.92      0.94        78\n",
      "           1       0.97      0.83      0.90        72\n",
      "           2       0.82      0.87      0.84        82\n",
      "           3       0.85      0.77      0.80        86\n",
      "           4       0.92      0.92      0.92        83\n",
      "           5       0.95      0.90      0.92        88\n",
      "           6       0.84      0.91      0.87        79\n",
      "           7       0.96      0.88      0.92        86\n",
      "           8       0.82      0.94      0.87        66\n",
      "           9       0.92      0.92      0.92        87\n",
      "          10       0.93      0.96      0.95       103\n",
      "          11       0.85      0.89      0.87        93\n",
      "          12       0.86      0.96      0.90        68\n",
      "          13       0.89      0.92      0.91        73\n",
      "          14       0.93      0.91      0.92        70\n",
      "          15       0.95      0.91      0.93        86\n",
      "          16       0.87      0.94      0.90        99\n",
      "          17       0.95      0.97      0.96        76\n",
      "          18       0.95      0.89      0.92        89\n",
      "          19       0.90      0.95      0.93        80\n",
      "          20       0.96      0.93      0.94        72\n",
      "          21       0.97      0.88      0.92        69\n",
      "          22       0.94      0.93      0.93        85\n",
      "          23       0.83      0.82      0.82        92\n",
      "          24       0.81      0.87      0.84        69\n",
      "          25       0.96      0.90      0.93        83\n",
      "\n",
      "    accuracy                           0.90      2114\n",
      "   macro avg       0.91      0.90      0.90      2114\n",
      "weighted avg       0.91      0.90      0.90      2114\n",
      "\n",
      "Fold 6: 0.905\n",
      "Media do modelo  0.9051402006156215\n",
      "\n",
      "Melhor parametro: {'hidden_layer_sizes': 100, 'learning_rate': 'constant', 'learning_rate_init': 0.0001}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.96      0.93        90\n",
      "           1       0.96      0.87      0.91        84\n",
      "           2       0.85      0.85      0.85        78\n",
      "           3       0.91      0.74      0.82        94\n",
      "           4       0.88      0.86      0.87        76\n",
      "           5       0.93      0.95      0.94        78\n",
      "           6       0.89      0.91      0.90        82\n",
      "           7       0.93      0.95      0.94        79\n",
      "           8       0.85      0.85      0.85        74\n",
      "           9       0.94      0.91      0.93        68\n",
      "          10       0.95      0.89      0.92        65\n",
      "          11       0.89      0.92      0.90        76\n",
      "          12       0.89      0.83      0.86        88\n",
      "          13       0.83      0.86      0.85        58\n",
      "          14       0.93      0.98      0.95        81\n",
      "          15       0.96      0.89      0.92        83\n",
      "          16       0.91      0.87      0.89        94\n",
      "          17       0.89      0.90      0.90        84\n",
      "          18       0.95      0.88      0.91        88\n",
      "          19       0.94      0.94      0.94        80\n",
      "          20       0.90      0.94      0.92        97\n",
      "          21       0.94      0.86      0.90        87\n",
      "          22       0.89      0.94      0.91        90\n",
      "          23       0.84      0.88      0.86        80\n",
      "          24       0.78      0.95      0.86        92\n",
      "          25       0.85      0.97      0.90        68\n",
      "\n",
      "    accuracy                           0.90      2114\n",
      "   macro avg       0.90      0.90      0.90      2114\n",
      "weighted avg       0.90      0.90      0.90      2114\n",
      "\n",
      "Fold 7: 0.899\n",
      "Media do modelo  0.8990549415985304\n",
      "\n",
      "Melhor parametro: {'hidden_layer_sizes': 100, 'learning_rate': 'constant', 'learning_rate_init': 0.0001}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.93      0.91        81\n",
      "           1       0.91      0.90      0.91        96\n",
      "           2       0.90      0.84      0.87        87\n",
      "           3       0.86      0.84      0.85        68\n",
      "           4       0.88      0.90      0.89        93\n",
      "           5       0.95      0.95      0.95        88\n",
      "           6       0.85      0.92      0.88        72\n",
      "           7       0.97      0.96      0.96        89\n",
      "           8       0.92      0.91      0.91        88\n",
      "           9       0.96      0.94      0.95        87\n",
      "          10       0.97      0.94      0.96        70\n",
      "          11       0.83      0.93      0.88        59\n",
      "          12       0.88      0.84      0.86        76\n",
      "          13       0.87      0.86      0.87        71\n",
      "          14       0.91      0.94      0.93       106\n",
      "          15       0.97      0.94      0.96        72\n",
      "          16       0.90      0.93      0.92        75\n",
      "          17       0.97      0.86      0.91        77\n",
      "          18       0.87      0.85      0.86        73\n",
      "          19       0.92      0.95      0.93        74\n",
      "          20       0.95      0.95      0.95        79\n",
      "          21       0.89      0.96      0.92        83\n",
      "          22       0.98      0.96      0.97        82\n",
      "          23       0.87      0.92      0.90        79\n",
      "          24       0.86      0.90      0.88        90\n",
      "          25       0.94      0.87      0.90        98\n",
      "\n",
      "    accuracy                           0.91      2113\n",
      "   macro avg       0.91      0.91      0.91      2113\n",
      "weighted avg       0.91      0.91      0.91      2113\n",
      "\n",
      "Fold 8: 0.912\n",
      "Media do modelo  0.9116192691325813\n",
      "\n",
      "Melhor parametro: {'hidden_layer_sizes': 100, 'learning_rate': 'constant', 'learning_rate_init': 0.0001}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.87      0.89        97\n",
      "           1       0.91      0.88      0.89        76\n",
      "           2       0.89      0.89      0.89        84\n",
      "           3       0.87      0.76      0.81        88\n",
      "           4       0.89      0.91      0.90        75\n",
      "           5       0.92      0.84      0.88        69\n",
      "           6       0.78      0.88      0.83        93\n",
      "           7       0.90      0.93      0.91        84\n",
      "           8       0.82      0.93      0.87        76\n",
      "           9       0.93      0.90      0.92        72\n",
      "          10       0.98      0.99      0.98        81\n",
      "          11       0.90      0.85      0.87        72\n",
      "          12       0.84      0.84      0.84        88\n",
      "          13       0.88      0.91      0.89        97\n",
      "          14       0.89      0.92      0.90        84\n",
      "          15       0.91      0.88      0.89        77\n",
      "          16       0.91      0.91      0.91        89\n",
      "          17       0.97      0.89      0.93        93\n",
      "          18       0.92      0.86      0.88        76\n",
      "          19       0.87      0.97      0.91        87\n",
      "          20       0.99      0.92      0.95        74\n",
      "          21       0.95      0.92      0.94        91\n",
      "          22       0.86      0.86      0.86        57\n",
      "          23       0.79      0.82      0.81        76\n",
      "          24       0.76      0.83      0.79        69\n",
      "          25       0.93      0.93      0.93        88\n",
      "\n",
      "    accuracy                           0.89      2113\n",
      "   macro avg       0.89      0.89      0.89      2113\n",
      "weighted avg       0.89      0.89      0.89      2113\n",
      "\n",
      "Fold 9: 0.890\n",
      "Media do modelo  0.8902834512990258\n",
      "\n",
      "Precisão média (desvio): 0.899 +- (0.006)\n"
     ]
    }
   ],
   "source": [
    "model = MLPClassifier()\n",
    "train_score = []\n",
    "val_score = []\n",
    "\n",
    "parameters = {'hidden_layer_sizes':n,\n",
    "              'learning_rate': learningRate,\n",
    "              'learning_rate_init': learningRateInit}\n",
    "\n",
    "grid = GridSearchCV(estimator = model,\n",
    "                    param_grid = parameters,\n",
    "                    scoring = 'precision_macro',\n",
    "                    cv = 2)\n",
    "\n",
    "def evaluate_model_with_kfold(kf):\n",
    "    precision_list = []\n",
    "    fold = 0\n",
    "    for train, test in kf.split(X_smt, y_smt):\n",
    "\n",
    "        y_train, y_test = y_smt.iloc[train], y_smt.iloc[test]\n",
    "\n",
    "\n",
    "        X_anova_train = fs.fit_transform(X_smt.iloc[train],y_train)\n",
    "        X_anova_test = fs.transform(X_smt.iloc[test])\n",
    "\n",
    "        grid.fit(X_anova_train, y_train)\n",
    "\n",
    "        y_pred = grid.predict(X_anova_test)\n",
    "\n",
    "        print(\"\\nMelhor parametro:\", grid.best_params_)\n",
    "        print(classification_report(y_test, grid.predict(X_anova_test)))\n",
    "        print(\"Fold %d: %.3f\" %(fold, precision_score(y_test, y_pred,average=\"macro\")))\n",
    "\n",
    "        precision_list.append(precision_score(y_test, y_pred,average=\"macro\"))\n",
    "        print(\"Media do modelo \", precision_list[fold].mean())\n",
    "\n",
    "        fold += 1\n",
    "\n",
    "    precision = np.array(precision_list)\n",
    "    print(\"\\nPrecisão média (desvio): %.3f +- (%.3f)\" %(precision.mean(), precision.std()))\n",
    "\n",
    "# chamada da função de Treino com KFOLD\n",
    "evaluate_model_with_kfold(kFoldObj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TREINAMENTOS DT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DT - PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DT - LCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DT - ANOVA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TREINAMENTOS NB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NB - PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NB - LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NB - ANOVA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TREINAMENTOS SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM - PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM - LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM - ANOVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
